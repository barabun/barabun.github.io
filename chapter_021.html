<?xml version='1.0' encoding='utf-8'?>
<!DOCTYPE html>
<html epub:prefix="z3998: http://www.daisy.org/z3998/2012/vocab/structure/#" lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
 <head>
  <link href="style.css" rel="stylesheet" type="text/css"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   12: Testing
  </title>
 </head>
 <body>
  <div class="navigation">
   <a class="nav-button" href="chapter_020.html">
    ← Previous
   </a>
   <a class="nav-button" href="index.html">
    Table of Contents
   </a>
   <a class="nav-button" href="chapter_022.html">
    Next →
   </a>
  </div>
  <div class="chapter-content">
   <div id="sbo-rt-content">
    <span aria-label="443" epub:type="pagebreak" id="pagebreak_443" role="doc-pagebreak">
    </span>
    <section epub:type="chapter" id="CH0012" role="doc-chapter" xmlns:epub="http://www.idpf.org/2007/ops">
     <h1 class="chaptitle" epub:type="title" id="B9780443222191000210">
      12: Testing
     </h1>
     <section epub:type="preamble">
      <a id="abs0010">
      </a>
      <div class="abstract">
       <h2 class="h1hd" id="cesectitle0010">
        Abstract
       </h2>
       <div id="abssec0010">
        <p class="abspara" id="abspara0010">
         Automated testing is one of the most important activities you can invest in to ensure that application programming interface (API) changes don't break your clients' programs. This chapter starts off by presenting the case for writing tests and then categorizes the various types of testing you can perform on an API, including open box testing and closed box testing, as well as unit testing, integration testing, and performance testing. The topic of how to write good tests is covered by detailing the qualities of a good test, what to test, how to focus your testing effort, and how to work effectively with a quality assurance team. The next topic looks at how you can write code that's more amenable to testing. This covers techniques such as test-driven development, stub objects and mock objects, how to test private code, the proper use of assertions, and internationalization issues. In the final section, various automated testing tools are presented, such as a range of test harnesses, code coverage tools, bug tracking systems, and continuous build systems.
        </p>
       </div>
      </div>
     </section>
     <section>
      <h3 class="h2hd" id="cesectitle0015">
       Keywords
      </h3>
      <div class="keywords">
       Assertion; Code coverage; Integration testing; Mock object; Performance testing; Quality assurance (QA); Stub; Test-driven development (TDD); Unit testing
      </div>
     </section>
     <section>
      <p class="textfl" id="p0010">
       Every developer, no matter how experienced and meticulous, will introduce bugs into the software they write. This is simply inevitable as an application programming interface (API) grows in size and complexity. The purpose of testing is to locate these defects as early as possible so they can be addressed before they affect your clients.
      </p>
      <p class="text" id="p0015">
       Modern software development relies heavily on the use of third-party APIs. As your own APIs become more ubiquitous, failures and defects in your code will have the potential to affect many clients and their applications.
      </p>
      <p class="text" id="p0020">
       As I noted earlier, your clients may eventually seek alternative solutions if the code you deliver is buggy or unpredictable, or crashes regularly. Conscientious testing is therefore a critical part of your API development process because it can be used to increase the reliability and stability of your product. This will ultimately contribute to the success of your API in the marketplace.
      </p>
      <div>
       <aside aria-labelledby="b0020" epub:type="sidebar">
        <div class="box_top_space">
        </div>
        <div class="boxg1" id="b0020">
         <div class="b1textfl" id="bpar0125">
          <i>
           TIP: Writing automated tests is the most important thing you can do to ensure that you don't break your users’ programs.
          </i>
         </div>
        </div>
       </aside>
      </div>
      <p class="text" id="p0030">
       In this chapter, I'll cover various types of automated testing that you can employ, such as unit testing, integration testing, and performance testing. I will also look at how to write good tests, as well as the equally important factor of how to design and implement APIs that are more amenable to testing. Finally, I'll complement this discussion by surveying some of the testing tools you can adopt for your project and look at how you can write tests using various popular automated testing frameworks. Along the way, I'll discuss process issues such as collaborating with a quality assurance (QA) team, using quality metrics, and how to integrate testing into your build process.
      </p>
      <section>
       <a id="s0010">
       </a>
       <h2 class="h1hd" id="cesectitle0020">
        Reasons to write tests
       </h2>
       <p class="textfl" id="p0035">
        It's a common fallacy that engineers don't like to write tests. From my experience, every good developer understands the need for testing, and most have encountered cases in which testing has caught a bug in their code. At the end of the day, software engineers are craftspeople who take pride in their work and will probably find it demoralizing to be expected to produce low-quality results. However, if the management for a project does not explicitly incorporate the need for testing into the schedule, then engineers are not given the resources and support they need to develop these tests. In general, a software
        <a id="p444">
        </a>
        <span aria-label="444" epub:type="pagebreak" id="pagebreak_444" role="doc-pagebreak">
        </span>
        project can be date-driven, quality-driven, or feature-driven. You can pick two of these, but not all three. For example, in the case of the waterfall development process, feature creep and unforeseen problems can easily eliminate any time at the end of a project that was reserved for testing. Consequently, an engineer who attempts to spend time writing automated tests can appear to be less productive within a date- or feature-driven process. Instead, if engineers are empowered to focus on quality, I believe that they will relish the opportunity to write tests for their code.
       </p>
       <p class="text" id="p0040">
        I experienced this firsthand at Pixar, where we decided to introduce a new policy that engineers had to write unit tests for their code, and furthermore, that all non-GUI code had to achieve 100% code coverage. That is, every line of non-GUI code had to be exercised from test code. Rather than incite a mass rebellion, we found that developers thought this was a good use of their time. The key enabling factor was that we added time to the end of each iteration when all developers could focus on writing tests for their code. Even after 2 years of following this policy, there was universal agreement that the benefits of writing tests outweighed the costs, and that maintaining a 100% coverage target was still appropriate.
       </p>
       <p class="text" id="p0045">
        In case you still need some incentive, here are some reasons why you should employ testing for your own projects:
       </p>
       <div>
        <ol id="ulist0010">
         <li class="bulllist" id="p0050">
          <a id="u0010">
          </a>
          ▪
          <b>
           Increased confidence
          </b>
          . Having an extensive suite of automated tests can give you the confidence to make changes to the behavior of an API with the knowledge that you're not breaking functionality. Said differently, testing can reduce your fear of implementing a change. It's common to find legacy systems in which engineers are uneasy changing certain parts of the code because those parts are so complex and opaque that changes to its behavior could have unforeseen consequences (
          <a href="../B9780443222191160015/References_583-587_B9780443222191160015.xhtml#bib29" id="bib_29">
           Feathers, 2004
          </a>
          ). Furthermore, the code in question may have been written by an engineer who is no longer with the organization, so there is no one who knows where the bodies are buried in the code.
         </li>
         <li class="bulllist" id="p0055">
          <a id="u0015">
          </a>
          ▪
          <b>
           Ensuring backward compatibility
          </b>
          . It's important to know that you haven't introduced changes in a new version of the API that break backward compatibility for code written against an older version of the API, or for data files generated by that older API. The use of automated tests can be used to capture the workflows and behavior from earlier versions so that these are always exercised in the latest version of the library.
         </li>
         <li class="bulllist" id="p0060">
          <a id="u0020">
          </a>
          ▪
          <b>
           Saving costs
          </b>
          . It's a well-known fact that fixing defects later in the development cycle is more expensive than fixing them earlier. This is because the defect becomes more deeply embedded in the code, and exorcising it may also involve updating data files. For example, Steve McConnell gives evidence that fixing a bug after release can be 10–25 times more expensive than during development (
          <a href="../B9780443222191160015/References_583-587_B9780443222191160015.xhtml#bib56" id="bib_56">
           McConnell, 2004
          </a>
          ). Developing a suite of automated tests lets you discover defects earlier so that they can be fixed earlier, and hence is more economically overall.
         </li>
         <li class="bulllist" id="p0065">
          <a id="u0025">
          </a>
          ▪
          <b>
           Codify uses cases
          </b>
          . The use cases for an API represent supported workflows that your users should be able to accomplish. Developing tests for these use cases before you implement your API can let you know when you've achieved the
          <a id="p445">
          </a>
          <span aria-label="445" epub:type="pagebreak" id="pagebreak_445" role="doc-pagebreak">
          </span>
          required functionality. These same tests can then be used continuously to catch any regressions in these important and key workflows.
         </li>
         <li class="bulllist" id="p0070">
          <a id="u0030">
          </a>
          ▪
          <b>
           Compliance assurance
          </b>
          . Software for use in certain safety- or security-critical applications may have to pass regulatory tests, such as Federal Aviation Administration certification. Also, some organizations may verify that your software conforms to their standard before allowing it to be branded as such. For example, the Open Geospatial Consortium (OGC) has a compliance testing program for software that is to be branded as Certified OGC Compliant. Automated tests can be used to ensure that you continue to conform to these regulatory and standards requirements.
         </li>
        </ol>
       </div>
       <p class="text" id="p0075">
        These points can be summarized by stating that automated testing can help you to determine whether you're building the right thing (referred to as validation), and if you're building it right (called verification).
       </p>
       <p class="text" id="p0080">
        There can be a downside to writing many tests. As the size of your test suite grows, the maintenance for these tests grows commensurately. This can result in situations where a good code change that takes a couple of minutes to make might also break hundreds of tests and require many hours of effort also to update the test suite. This is a bad situation to get into because it disincentivizes an engineer from making a good fix only because of the overhead of updating the tests. I'll discuss ways to avoid this situation in the following sections. However, if the fix in question changes the public API, then the extra barrier may be a good thing because it forces the engineer to consider the potential impact on backward compatibility for existing clients.
       </p>
       <div>
        <aside aria-labelledby="b0010" epub:type="sidebar">
         <div class="box_top_space">
         </div>
         <div class="boxg1" id="b0010">
          <p>
          </p>
          <div class="b1title" epub:type="title" id="title0015">
           <i>
            SIDEBAR: The Cost of Untested Code
           </i>
          </div>
          <div class="b1textfl" id="bpar0010">
           <i>
            There are many examples of catastrophic software failures that have happened over the years. Numerous websites maintain catalogs of these bugs. In many of these cases, more thorough testing would have averted disaster. For example:
           </i>
          </div>
          <div class="b1text" id="bpar0015">
           <i>
            In May 1996, a software bug caused the bank accounts of 823 customers of a major US bank to be credited with $924,844,208.32 each. The American Bankers Association described this as the largest error in US banking history. It happened because the bank added new message codes to its ATM transaction software but failed to test them on all ATM protocols.
           </i>
          </div>
          <div class="b1text" id="bpar0020">
           <i>
            As an example of the need for backward compatibility testing, the Tokyo stock exchange halted trading for most of a day during Nov. 2005 owing to a software glitch after a systems upgrade.
           </i>
          </div>
          <div class="b1text" id="bpar0025">
           <i>
            Failing to meet client requirements can have a large impact on your business. For example, in Nov. 2007, a regional government in the United States brought a multimillion-dollar lawsuit against a software services vendor because the criminal justice information software they delivered “minimized quality” and did not meet requirements.
           </i>
          </div>
          <div class="b1text" id="bpar0030">
           <i>
            Finally, as an example of the importance of compliance testing, in Jan. 2009, regulators banned a large US health insurance company from selling Medicare programs because of computer errors that posed “a serious threat to the health and safety of Medicare beneficiaries.”
           </i>
          </div>
         </div>
        </aside>
       </div>
       <p class="textfl">
        <a id="p446">
        </a>
       </p>
       <div>
        <span aria-label="446" epub:type="pagebreak" id="pagebreak_446" role="doc-pagebreak">
        </span>
       </div>
      </section>
      <section>
       <a id="s0015">
       </a>
       <h2 class="h1hd" id="cesectitle0025">
        Types of API testing
       </h2>
       <p class="textfl" id="p0085">
        Testing an API is different from testing an end user or GUI application. However, there are still various techniques that are applicable to both cases. For example, a common categorization of software testing methodologies is:
       </p>
       <div>
        <ul class="ce_list" id="olist0010">
         <li class="numlist" id="p0090">
          <a id="o0010">
          </a>
          1.
          <b>
           Open box testing
          </b>
          : Tests are developed with knowledge of the source code implementation and are normally automated using a programming language. This is sometimes called white box or clear box testing, implying that the tester can view the inner workings of the system.
         </li>
         <li class="numlist" id="p0095">
          <a id="o0015">
          </a>
          2.
          <b>
           Closed box testing
          </b>
          : Tests are developed without knowledge of the underlying implementation and are instead based upon specifications such as functional requirements and use cases. This is sometimes called black box testing, meaning that the inner workings are hidden from the tester.
         </li>
        </ul>
       </div>
       <p class="text" id="p0100">
        These terms can sometimes imply the use or nonuse of automation. For example, because open box testing generally involves writing code, it can be easy to automate these tests and run them automatically as part of a build process, whereas closed box testing is often associated with manual testing, in which a person walks through a series of manual steps to exercise a program physically from the point of view of a user. This doesn't always have to be the case, though: closed box testing could be automated, too. From an API testing perspective, you can imagine some automated tests being written with knowledge of the implementation and others being written only from the point of view of your clients' expectations with no knowledge of how the features were implemented. There are, however, several types of automated software testing techniques that are not applicable to API testing.
       </p>
       <p class="text" id="p0105">
        For example, the term system testing refers to testing that's performed on a complete integrated system. This is normally assumed to be an actual application that can be run by a user. Although it's conceivable to consider a large API as a complete integrated system, I will instead subscribe to the view that an API is a building block or component that's used to build entire systems. As such, I will not consider system testing to be part of the tasks involved in testing an API.
       </p>
       <p class="text" id="p0110">
        Furthermore, the area of automated GUI testing is generally inappropriate for APIs, i.e. the task of writing automated scripts that run an end user application and simulate user interactions, such as clicking on buttons or typing text. The exception to this would be if you're writing a GUI toolkit that creates these button and text entry widgets. However, in this case, you (and your clients) would be well-served by creating a custom testing tool that can navigate and interact with your widget hierarchy for the purposes of automated testing. For example, Froglogic provides an automated GUI testing tool for Qt applications called Squish.
       </p>
       <p class="text" id="p0115">
        Consequently, the primary functional testing strategies I'll concentrate on here are unit testing and integration testing. Unit testing verifies that the software does what the programmer expects, whereas integration testing satisfies clients that the software addresses their needs. You can also write tests to verify the nonfunctional requirements of your API. The subsequent list provides a selection of some of the most common
        <a id="p447">
        </a>
        <span aria-label="447" epub:type="pagebreak" id="pagebreak_447" role="doc-pagebreak">
        </span>
        nonfunctional testing strategies (I'll cover the topic of performance testing later in this chapter to provide an example of nonfunctional testing):
       </p>
       <div>
        <ol id="ulist0015">
         <li class="bulllist" id="p0120">
          <a id="u0035">
          </a>
          ▪
          <b>
           Performance testing
          </b>
          : Verifies that the functionality of your API meets minimum speed or memory usage requirements.
         </li>
         <li class="bulllist" id="p0125">
          <a id="u0040">
          </a>
          ▪
          <b>
           Load testing
          </b>
          : Puts demand, or stress, on a system and measures its ability to handle this load. This often refers to testing with many simultaneous users or performing many API requests per second. This is sometimes also called stress testing.
         </li>
         <li class="bulllist" id="p0130">
          <a id="u0045">
          </a>
          ▪
          <b>
           Scalability testing
          </b>
          : Ensures that the system can handle large and complex production data inputs instead of only simple test datasets. This is sometimes also called capacity or volume testing.
         </li>
         <li class="bulllist" id="p0135">
          <a id="u0050">
          </a>
          ▪
          <b>
           Soak testing
          </b>
          : Attempts to run the software continuously over an extended period to satisfy clients that it is robust and can handle sustained use (e.g., that there are no major memory leaks).
         </li>
         <li class="bulllist" id="p0140">
          <a id="u0055">
          </a>
          ▪
          <b>
           Security testing
          </b>
          : Ensures that any security requirements of your code are met, such as the confidentiality, authentication, authorization, integrity, and availability of sensitive information.
         </li>
         <li class="bulllist" id="p0145">
          <a id="u0060">
          </a>
          ▪
          <b>
           Concurrency testing
          </b>
          : Verifies the multithreaded behavior of your code to ensure that it behaves correctly and does not deadlock.
         </li>
        </ol>
       </div>
       <div>
        <aside aria-labelledby="b0025" epub:type="sidebar">
         <div class="box_top_space">
         </div>
         <div class="boxg1" id="b0025">
          <div class="b1textfl" id="bpar0125a">
           <i>
            TIP: API testing should involve a combination of unit and integration testing. No-functional techniques can also be applied as appropriate, such as performance, concurrency, and security testing.
           </i>
          </div>
         </div>
        </aside>
       </div>
       <section>
        <a id="s0020">
        </a>
        <h3 class="h2hd" id="cesectitle0030">
         Unit testing
        </h3>
        <p class="textfl" id="p0155">
         A unit test is used to verify a single minimal unit of source code, such as an individual method or class. The purpose of unit testing is to take the smallest testable parts of an API and verify that they function correctly in isolation.
        </p>
        <p class="text" id="p0160">
         These kinds of tests tend to run very fast and often take the form of a sequence of assertions that return true or false, in which any false result will fail the test. Often these tests are colocated with the code that they test (such as in a
         <span class="inlinecode">
          tests
         </span>
         subdirectory) and they can be compiled and run at the same point that the code itself is compiled. Unit tests tend to be written by developers using knowledge of the implementation; as such, they are an open box testing technique.
        </p>
        <div>
         <aside aria-labelledby="b0030" epub:type="sidebar">
          <div class="box_top_space">
          </div>
          <div class="boxg1" id="b0030">
           <div class="b1textfl" id="bpar0125j">
            <i>
             TIP: Unit testing is an open box testing technique to verify the behavior of functions and classes in isolation.
            </i>
           </div>
          </div>
         </aside>
        </div>
        <p class="textfl">
         <a id="p448">
         </a>
        </p>
        <div>
         <span aria-label="448" epub:type="pagebreak" id="pagebreak_448" role="doc-pagebreak">
         </span>
        </div>
        <p class="text" id="p0170">
         To give a concrete example of a unit test, let's consider a function you want to test that converts a string to a double:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0025">
           <img alt="image" height="81" src="../../IMAGES/B9780443222191000210/main.assets/u12-01-9780443222191.jpg" width="2079"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0175">
         This function accepts a string parameter and returns a Boolean to indicate whether the conversion was successful. If successful, the double value is written to the
         <span class="inlinecode">
          result
         </span>
         reference parameter. Given this function, the next unit test performs a series of checks to ensure that it works as expected:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0030">
           <img alt="image" height="1850" src="../../IMAGES/B9780443222191000210/main.assets/u12-02-9780443222191.jpg" width="2157"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0180">
         Note the use of various helper functions to test the result of each operation:
         <span class="inlinecode">
          Assert()
         </span>
         ,
         <span class="inlinecode">
          AssertFalse()
         </span>
         , and
         <span class="inlinecode">
          AssertEqual()
         </span>
         . These are common functions in unit test frameworks that follow the JUnit style, although sometimes other, similar function names or capitalizations are used. If any of these JUnit-style assertions fail, then the entire test will fail, normally with a descriptive error message that pinpoints the failure.
         <a id="p449">
         </a>
         <span aria-label="449" epub:type="pagebreak" id="pagebreak_449" role="doc-pagebreak">
         </span>
        </p>
        <div>
         <aside aria-labelledby="b0015" epub:type="sidebar">
          <div class="box_top_space">
          </div>
          <div class="boxg1" id="b0015">
           <p>
           </p>
           <div class="b1title" epub:type="title" id="title0020">
            <i>
             SIDEBAR: JUnit
            </i>
           </div>
           <div class="b1textfl" id="bpar0035">
            <i>
             JUnit is a unit testing framework originally developed for the Java programming language by Kent Beck and Erich Gamma. It's designed around two key design patterns: Command and Composite.
            </i>
           </div>
           <div class="b1text" id="bpar0040">
            <i>
             Each test case is a command object that defines one or more
            </i>
            <span class="inlinecode">
             <i>
              test∗()
             </i>
            </span>
            <i>
             methods, such as
            </i>
            <span class="inlinecode">
             <i>
              testMyObject()
             </i>
            </span>
            <i>
             , as well as optional
            </i>
            <span class="inlinecode">
             <i>
              setUp()
             </i>
            </span>
            <i>
             and
            </i>
            <span class="inlinecode">
             <i>
              tearDown()
             </i>
            </span>
            <i>
             methods. Multiple test cases can be collated within a test suite. The test suite is a composite of test cases that automatically calls all of the
            </i>
            <span class="inlinecode">
             <i>
              test∗()
             </i>
            </span>
            <i>
             methods.
            </i>
           </div>
           <div class="b1text" id="bpar0045">
            <i>
             JUnit also provides a number of methods to support assertion-based testing, such as
            </i>
            <span class="inlinecode">
             <i>
              assertEquals()
             </i>
            </span>
            <i>
             ,
            </i>
            <span class="inlinecode">
             <i>
              assertNull()
             </i>
            </span>
            <i>
             ,
            </i>
            <span class="inlinecode">
             <i>
              assertTrue()
             </i>
            </span>
            <i>
             , and
            </i>
            <span class="inlinecode">
             <i>
              assertSame()
             </i>
            </span>
            <i>
             . If any of these is passed an expression that evaluates to false, then the test case will be marked as failed.
            </i>
           </div>
           <div class="b1text" id="bpar0050">
            <i>
             Since the initial popularity of JUnit, the framework has been ported to many other languages and has become known by the more general moniker of xUnit. For example, there is PyUnit for Python, CUnit for C, and CppUnit for C++, among many other implementations.
            </i>
           </div>
          </div>
         </aside>
        </div>
        <p class="text" id="p0185">
         That example is intentionally simple. However, in real software the method or object under test often depends upon other objects in the system or upon external resources such as files on disk, records in a database, or software on a remote server. This leads to two different views of unit testing:
        </p>
        <div>
         <ul class="ce_list" id="olist0015">
          <li class="numlist" id="p0190">
           <a id="o0020">
           </a>
           1.
           <b>
            Fixture setup
           </b>
           . The classic approach to unit testing is to initialize a consistent environment, or fixture, before each unit test is run: for example, to ensure that dependent objects and singletons are initialized, to copy a specific set of files to a known location, or to load a database with a prepared set of initial data. This is often done in a
           <span class="inlinecode">
            setUp()
           </span>
           function associated with each test, to differentiate the test setup steps from the actual test operations. A related
           <span class="inlinecode">
            tearDown()
           </span>
           function is often used to clean up the environment once the test finishes. One of the benefits of this approach is that the same fixture can often be reused for many tests.
          </li>
          <li class="numlist" id="p0195">
           <a id="o0025">
           </a>
           2.
           <b>
            Stub/mock objects
           </b>
           . In this approach, the code under test is isolated from the rest of the system by creating stub or mock objects that stand in for any dependencies outside the unit (
           <a href="../B9780443222191160015/References_583-587_B9780443222191160015.xhtml#bib57" id="bib_57">
            Mackinnon et al., 2001
           </a>
           ). For example, if a unit test needs to communicate with a database, a stub database object can be created that accepts the subset of queries that the unit will generate and then return canned data in response, without making an actual connection to the database. The result is a completely isolated test that will not be affected by database problems, network issues, or file system permissions. The downside, however, is that the creation of these stub objects can be tedious and often cannot be reused by other unit tests. On the other hand, mock objects tend to be more flexible and can be customized for individual tests. I'll discuss each of these options in more detail later in the chapter.
          </li>
         </ul>
        </div>
        <div>
         <aside aria-labelledby="b0070" epub:type="sidebar">
          <div class="box_top_space">
          </div>
          <div class="boxg1" id="b0070">
           <div class="b1textfl" id="bpar0125b">
            <i>
             TIP: If your code depends upon an unreliable resource, such as a database, file system, or network, consider using stub or mock objects to produce more robust unit tests.
            </i>
           </div>
          </div>
         </aside>
        </div>
        <p class="textfl">
         <a id="p450">
         </a>
        </p>
        <div>
         <span aria-label="450" epub:type="pagebreak" id="pagebreak_450" role="doc-pagebreak">
         </span>
        </div>
       </section>
       <section>
        <a id="s0025">
        </a>
        <h3 class="h2hd" id="cesectitle0035">
         Integration testing
        </h3>
        <p class="textfl" id="p0205">
         In contrast to unit testing, integration testing is concerned with the interaction of several components in cooperation. Ideally, the individual components have already been unit tested. This form of testing can also be called component testing.
        </p>
        <p class="text" id="p0210">
         Integration tests are still necessary even if you have a high degree of unit test coverage, because testing individual units of code in isolation does not guarantee that they can be used together easily and efficiently, or that they meet your functional requirements and use cases. For example, the interface of one component may be incompatible with another component, or information required by another component may not be appropriately exposed for another component to use. The goal of integration testing is therefore to ensure that all of the components of your API work well together and are consistent, and that they enable users to perform the tasks they need.
        </p>
        <p class="text" id="p0215">
         Integration tests are normally developed against the specification of the API, such as the automatically generated API documentation, and therefore should not require an understanding of the internal implementation details. That is, they are written from the perspective of your clients. As such, integration testing is a closed box testing technique.
        </p>
        <div>
         <aside aria-labelledby="b0035" epub:type="sidebar">
          <div class="box_top_space">
          </div>
          <div class="boxg1" id="b0035">
           <div class="b1textfl" id="bpar0125y">
            <i>
             TIP: Integration testing is a closed box testing technique to verify the interaction of several components together.
            </i>
           </div>
          </div>
         </aside>
        </div>
        <p class="text" id="p0225">
         You can often use the same tools to implement integration tests that you use to write unit tests. However, integration tests usually involve more complex ways to verify that a sequence of operations was successful. For example, a test may generate an output file that must be compared against a golden or baseline version that's stored with the test in your revision control system. This requires an efficient workflow to update the baseline version in cases where the failure is expected, such as the conscious addition of new elements in the data file or changes to the behavior of a function to fix a bug.
        </p>
        <p class="text" id="p0230">
         A good integration testing framework will therefore include dedicated comparison functions (or diff commands) for each file type that the API can produce. For example, an API may have an ASCII configuration file, in which an integration test failure should be triggered only if the value or number of settings changes, but not if the order of the settings in the file changes or if different whitespace characters are used to separate settings. As another example, an API may produce an image as its result. You therefore need a way to compare the output image against a baseline version of that image. For example, the R&amp;D team at PDI/Dreamworks developed a perceptual image difference utility to verify that the rendered images for their film assets are visibly the same after a change to their animation system. This perceptually based comparison allows for minor imperceptible differences in the actual pixel values to avoid unnecessary failures (
         <a href="../B9780443222191160015/References_583-587_B9780443222191160015.xhtml#bib104" id="bib_104">
          Yee and Newman, 2004
         </a>
         ).
        </p>
        <p class="text" id="p0235">
         This last example brings up the point that integration testing may also be data driven. That is, a single test program can be called many times with different input data. For
         <a id="p451">
         </a>
         <span aria-label="451" epub:type="pagebreak" id="pagebreak_451" role="doc-pagebreak">
         </span>
         example, a C++ parser may be verified with a single integration test that reads a
         <span class="inlinecode">
          .cpp
         </span>
         source file and outputs its derivation or abstract syntax tree. That test can then be called many times with different C++ source programs and its output compared against a correct baseline version in each case. Similarly, the libpng library has a
         <span class="inlinecode">
          pngtest.c
         </span>
         program that reads an image and then writes it out again. This test is then run in a data-driven fashion using Willem van Schaik's suite of representative PNG images called PngSuite.
         <a href="#f0010" id="Bf0010">
          Fig. 12.1
         </a>
         shows a few of the images in PngSuite. This integration test ensures that new changes to libpng don't break its ability to read and write various combinations of the PNG file format, including basic chunk handling, compression, interlacing, alpha transparency, filtering, gamma, and image comments, among other attributes.
        </p>
        <div class="pageavoid">
         <figure class="fig" id="f0010">
          <img alt="image" height="1170" src="../../IMAGES/B9780443222191000210/main.assets/f12-01-9780443222191.jpg" width="2730"/>
          <figcaption class="figleg">
           <a id="cap0010">
           </a>
           <a id="fspara0010">
           </a>
           <span class="fignum">
            <a href="#Bf0010">
             Figure 12.1
            </a>
           </span>
           A subset of Willem van Schaik's PNG image test suite, called PngSuite. See
           <a href="http://www.schaik.com/">
            http://www.schaik.com/
           </a>
           for details.
          </figcaption>
         </figure>
        </div>
        <p class="text" id="p0240">
         Sometimes your API may be supported by one or more command line utilities to allow the functionality of your API to be accessed from the terminal or shell scripts. For example, the libjpeg library ships with various utility programs such as
         <span class="inlinecode">
          cjpeg
         </span>
         ,
         <span class="inlinecode">
          djpeg
         </span>
         , and
         <span class="inlinecode">
          jpegtran
         </span>
         . You should write automated tests for these tools as well because they are part of your SDK, too, and it can often be easy to break these tools without knowing it. You should therefore have tests in which you execute these tools with various command line arguments and then validate their outputs. Ideally, you would validate the success of these tests based on the results they produce, such as whether a new file was generated correctly on disk. However sometimes you may need to parse the stdout or stderr of the command to achieve what you need. Just be aware in that case you're building more brittle tests in which an engineer could easily edit an error message and break your tests.
        </p>
        <p class="text" id="p0245">
         Integration testing of APIs can be performed by developers, but in larger organizations it can also be a task that your QA team performs. In fact, a QA engineer will probably refer to this activity as API testing, which is a term that often implies ownership by QA. I've avoided using the specific term API testing here simply because this entire chapter is about testing APIs. Moreover, some of the examples I gave earlier, such as
         <a id="p452">
         </a>
         <span aria-label="452" epub:type="pagebreak" id="pagebreak_452" role="doc-pagebreak">
         </span>
         automated testing of command line utilities, could be done by QA engineers who don't know C++ or who focus more on writing shell scripts than compiled code.
        </p>
        <p class="text" id="p0250">
         Given that integration tests have a different focus than unit tests, may be maintained by a different team, and normally must be run after the build has completed successfully, these kinds of tests are commonly located in a different directory than unit tests. For example, they may live in a sibling directory to the top-level source directory, rather than being stored next to the actual code inside the source directory. This strategy also reflects the closed box nature of integration tests compared with open box unit tests.
        </p>
       </section>
       <section>
        <a id="s0030">
        </a>
        <h3 class="h2hd" id="cesectitle0040">
         Performance testing
        </h3>
        <p class="textfl" id="p0255">
         Typically, your users will demand a reasonable level of performance from your API. For instance, if you've written a library that provides real-time collision detection between 3D objects for a game engine, your implementation must run fast enough during each frame that it doesn't slow your clients' games. You could therefore write a performance test for your collision detection code in which the test will fail if it exceeds a predefined performance threshold.
        </p>
        <p class="text" id="p0260">
         As a further example, when Apple was developing their Safari Web browser, page rendering speed was of paramount concern. They therefore added performance tests and defined acceptable speed thresholds for each test. They then put a process in place in which a code checkin would be rejected if it caused a performance test to exceed its threshold. The engineer would have to optimize the code (or somebody else's code if their code was already optimal) before it could be checked in.
        </p>
        <div>
         <aside aria-labelledby="b0090" epub:type="sidebar">
          <div class="box_top_space">
          </div>
          <div class="boxg1" id="b0090">
           <div class="b1textfl" id="bpar0125h">
            <i>
             TIP: Performance testing of your key use cases helps you avoid introducing speed or memory regressions unknowingly.
            </i>
           </div>
          </div>
         </aside>
        </div>
        <p class="text" id="p0270">
         A related issue is that of stress testing, in which you verify that your implementation can scale to the real-world demands of your users: for example, a website that can handle many simultaneous users, or a particle system that can handle thousands or millions of particles. These are classed as nonfunctional tests because they don't test the correctness of a specific feature of your API, but instead are concerned with its operational behavior in the user's environment. That is, they test the nonfunctional requirements of your API.
        </p>
        <p class="text" id="p0275">
         The benefit of writing automated performance tests is that you can make sure any new changes don't adversely affect performance. For example, a senior engineer with whom I worked once refactored a data loading API to use an
         <span class="inlinecode">
          std::string
         </span>
         object instead of a
         <span class="inlinecode">
          char
         </span>
         buffer to store successive characters read from a data file. When the change was released, users found that the system took over 10 times longer to load their data files. It turns out that the
         <span class="inlinecode">
          std::string::append()
         </span>
         method was reallocating the string each time, growing it by a single byte on each call and hence causing massive amounts of memory
         <a id="p453">
         </a>
         <span aria-label="453" epub:type="pagebreak" id="pagebreak_453" role="doc-pagebreak">
         </span>
         allocations to happen. This was ultimately fixed by using an
         <span class="inlinecode">
          std::vector&lt;char&gt;
         </span>
         because the
         <span class="inlinecode">
          append()
         </span>
         method for that container behaved more optimally. A performance test that monitored the time to load large data files could have discovered this regression before it was released to clients.
        </p>
        <div>
         <aside aria-labelledby="b0100" epub:type="sidebar">
          <div class="box_top_space">
          </div>
          <div class="boxg1" id="b0100">
           <div class="b1textfl" id="bpar0125z">
            <i>
             TIP: If performance is important for your API, consider writing performance tests for your key uses cases to avoid unwittingly introducing performance regressions.
            </i>
           </div>
          </div>
         </aside>
        </div>
        <p class="text" id="p0285">
         However, performance tests tend to be much more difficult to write and maintain than unit or integration tests. One reason is that performance test results are real (floating-point) numbers that can vary from run to run. They're not discrete true or false values. It's therefore advisable to specify a tolerance for each test to deal with the variability of each test run. For example, you might specify 10
         <span title='hsp="0.25"'>
         </span>
         ms as the threshold for your collision detection algorithm but allow for a 15% fluctuation before marking the test as failed. Another technique is to fail the test only after several consecutive data points exceed the threshold, to factor out anomalous spikes in performance.
        </p>
        <p class="text" id="p0290">
         Also, it's best to have dedicated hardware for running your performance tests, so that other processes running on the machine don't interfere with the test results. Even with a dedicated machine, you may need to investigate turning off certain system background processes so that they don't affect your timings. This reveals another reason why performance tests are difficult to maintain: they are machine specific. This implies that you need to store different threshold values for each class of machine on which you run the tests.
        </p>
        <p class="text" id="p0295">
         A further complication of performance testing is the problem of information overload. You may end up with hundreds or even thousands of combinations of each performance test on different hardware, each producing a multitude of data points throughout a single day. As a result, you will want to store all of your performance results in a database. Also, if you don't have automatic measures to highlight tests that have exceeded their performance threshold, then you may never notice regressions. On the other hand, with so many tests, you will likely be inundated with false positives and spend most of your time updating baseline values. At this point, you may have more success considering the issue to be a data mining problem. In other words, collect as much data as possible and then have regular database searches that pick the top 5 or 10 most egregious changes in performance and flag those for investigation by a human.
        </p>
        <p class="text" id="p0300">
         Mozilla offers a great example of extensive performance testing. They've implemented a system in which performance tests are run for multiple products across a range of hardware. The results can be browsed with an interactive website that displays graphs for one or more performance tests at the same time.
         <a href="#f0015" id="Bf0015">
          Fig. 12.2
         </a>
         shows an example of Facebook page load times for different browsers. (One thing to look out for when reading performance graphs is whether the y-axis starts at zero. If the results are scaled vertically to fit the screen, then what looks like a large degree of fluctuation could in reality be a tiny overall percentage change.)
         <a id="p454">
         </a>
        </p>
        <div>
         <span aria-label="454" epub:type="pagebreak" id="pagebreak_454" role="doc-pagebreak">
         </span>
        </div>
        <section>
         <a id="sf0015">
         </a>
         <div class="pageavoid">
          <figure class="fig" id="f0015">
           <img alt="image" height="1164" src="../../IMAGES/B9780443222191000210/main.assets/f12-02-9780443222191.jpg" width="2738"/>
           <figcaption class="figleg">
            <a id="cap0015">
            </a>
            <a id="fspara0015">
            </a>
            <span class="fignum">
             <a href="#Bf0015">
              Figure 12.2
             </a>
            </span>
            Firefox performance dashboard from
            <a href="https://arewefastyet.com/">
             https://arewefastyet.com/
            </a>
            .
           </figcaption>
          </figure>
         </div>
        </section>
       </section>
      </section>
      <section>
       <a id="s0035">
       </a>
       <h2 class="h1hd" id="cesectitle0045">
        Writing good tests
       </h2>
       <p class="textfl" id="p0305">
        Now that I've covered the basic types of API testing, I'll concentrate on how to write these automated tests. I will cover the qualities that make a good test and also present standard techniques for writing efficient and thorough tests. I'll also discuss how testing can be shared effectively with a QA team.
       </p>
       <section>
        <a id="s0040">
        </a>
        <h3 class="h2hd" id="cesectitle0050">
         Qualities of a good test
        </h3>
        <p class="textfl" id="p0310">
         Before I discuss the details of writing an automated test, I'll present a few high-level attributes of a good test. These are general qualities that you should always bear in mind when building your test suite. The overall message, however, is that you should treat test code with the same exacting standards that you use in your main API code. If you develop tests that exhibit these qualities, then you should end up with an easy to maintain and robust test suite that provides you with a valuable safety net for your API development:
        </p>
        <div>
         <ol id="ulist0020">
          <li class="bulllist" id="p0315">
           <a id="u0065">
           </a>
           •
           <b>
            Fast
           </b>
           . Your suite of tests should run quickly so that you get rapid feedback on test failures. Unit tests should always be fast, on the order of fractions of a second per test. Integration tests that perform actual user workflows, or data-driven integration tests that are run on many input files may take longer to execute. However, there are several ways to deal with this, such as favoring the creation of many unit tests but a few targeted integration tests. Also, you can have different categories of tests: the fast (or checkin or continuous) tests that are run during every build cycle, versus the slow (or complete or acceptance) tests that are run only occasionally, such as before a release.
           <a id="p455">
           </a>
          </li>
          <li>
           <span aria-label="455" epub:type="pagebreak" id="pagebreak_455" role="doc-pagebreak">
           </span>
          </li>
          <li class="bulllist" id="p0320">
           <a id="u0070">
           </a>
           •
           <b>
            Stable
           </b>
           . Tests should be repeatable, independent, and consistent: every time you run a specific version of a test you should get the same result. If a test starts failing erroneously or erratically, then your faith in the validity of that test's results will be diminished. You may even be tempted to turn the test off temporarily, which of course defeats the purpose of having the test. Using mock objects, in which all dependencies of a unit test are simulated, is one way to produce tests that are independent and stable to environmental conditions. It's also the only practical way to test date- or time-dependent code.
          </li>
          <li class="bulllist" id="p0325">
           <a id="u0075">
           </a>
           •
           <b>
            Portable
           </b>
           . If your API is implemented on multiple platforms, your tests should work across the same range of platforms. One of the most common areas of difference for test code running on different platforms is floating point comparisons. Rounding errors, architecture differences, and compiler differences can cause mathematical operations to produce slightly different results on different platforms. Floating point comparisons should therefore allow for a small error, or epsilon, rather than being compared exactly. This epsilon should be relative to the magnitude of the numbers involved and the precision of the floating-point type used. For instance, single-precision floats can represent only six to seven digits of precision. Therefore, an epsilon of 0.000001 may be appropriate when comparing numbers such as 1.234567, but an epsilon of 0.1 would be more appropriate when comparing numbers such as 123456.7.
          </li>
          <li class="bulllist" id="p0330">
           <a id="u0080">
           </a>
           •
           <b>
            High coding standards
           </b>
           . Test code should follow the same coding standards as the rest of your API: you should not slip standards just because the code will not be run directly by your users. Tests should be well-documented so that it's clear what's being tested and what a failure would imply. If you enforce code reviews for your API code, you should do the same for test code. In some places where I've worked, engineers prefer to start a code review by looking at unit test changes first. Similarly, you should not abandon your good engineering instincts simply because you are writing a test. If there's a case for factoring out common test code into a reusable test library, then you should do this. As the size of your test suite grows, you could end up with hundreds or thousands of tests. The need for robust and maintainable test code is therefore just as imperative as for your main API code.
          </li>
          <li class="bulllist" id="p0335">
           <a id="u0085">
           </a>
           •
           <b>
            Reproducible failure
           </b>
           . If a test fails, it should be easy to reproduce the failure. This means logging as much information as possible about the failure, pinpointing the actual point of failure as accurately as possible and making it easy for a developer to run the failing test in a debugger. Some systems employ randomized testing (called ad hoc testing), in which the test space is so large that random samples are chosen. In these cases, you should ensure that it's easy to reproduce the specific conditions that caused the failure, because simply rerunning the test will pick another random sample and may pass.
          </li>
         </ol>
        </div>
        <p class="textfl">
         <a id="p456">
         </a>
        </p>
        <div>
         <span aria-label="456" epub:type="pagebreak" id="pagebreak_456" role="doc-pagebreak">
         </span>
        </div>
       </section>
       <section>
        <a id="s0045">
        </a>
        <h3 class="h2hd" id="cesectitle0055">
         What to test
        </h3>
        <p class="textfl" id="p0340">
         Finally, we get to the part about actually writing tests. The way you write a unit test is different from the way you write an integration test. This is because unit tests can have knowledge about the internal structure of the code, such as loops and conditions. However, in both cases the aim is to exercise the capabilities of the API methodically. To this end, there is a range of standard QA techniques that you can employ to test your API. A few of the most pertinent ones are:
        </p>
        <div>
         <ol id="ulist0025">
          <li class="bulllist" id="p0345">
           <a id="u0090">
           </a>
           •
           <b>
            Condition testing
           </b>
           . When writing unit tests, you should use your knowledge of the code under test to exercise all combinations of any
           <span class="inlinecode">
            if
           </span>
           /
           <span class="inlinecode">
            else
           </span>
           ,
           <span class="inlinecode">
            for
           </span>
           ,
           <span class="inlinecode">
            while
           </span>
           , and
           <span class="inlinecode">
            switch
           </span>
           expressions within the unit. This ensures that all possible paths through the code have been tested. (I will discuss the details of statement coverage vs. decision coverage later in the chapter when I look at code coverage tools.)
          </li>
          <li class="bulllist" id="p0350">
           <a id="u0095">
           </a>
           •
           <b>
            Equivalence classes
           </b>
           . An equivalence class is a set of test inputs that all have the same expected behavior. The technique of equivalence class partitioning therefore attempts to find test inputs that exercise difference classes of behavior. For example, consider a square root function that is documented to accept values in the range 0–65,535. In this case there are three equivalence classes: the negative numbers, the valid range of numbers, and numbers greater than 65,535. You should therefore test this function with values from each of these three equivalence classes (e.g.,
           <span>
            −
           </span>
           10, 100, 100,000).
          </li>
          <li class="bulllist" id="p0355">
           <a id="u0100">
           </a>
           •
           <b>
            Boundary conditions
           </b>
           . Most errors occur around the boundary of expected values. How many times have you inadvertently written code with an off-by-one error? Boundary condition analysis focuses test cases around these boundary values. For example, if you're testing a routine that inserts an element into a linked list of length
           <i>
            n
           </i>
           , you should test inserting at position 0, 1,
           <i>
            n
           </i>
           <span>
            −
           </span>
           1, and
           <i>
            n.
           </i>
          </li>
          <li class="bulllist" id="p0360">
           <a id="u0105">
           </a>
           •
           <b>
            Parameter testing
           </b>
           . A test for a given API call should vary all parameters to the function to verify the full range of functionality. For example, the
           <span class="inlinecode">
            stdio.h
           </span>
           function
           <span class="inlinecode">
            fopen()
           </span>
           accepts a second argument to specify the file mode. This can take the values “r,” “w,” and “a,” in addition to optional “+” and “b” characters in each case. A thorough test for this function should therefore test all 12 combinations of the mode parameter to verify the full breadth of behavior.
          </li>
          <li class="bulllist" id="p0365">
           <a id="u0110">
           </a>
           •
           <b>
            Return value assertion
           </b>
           . This form of testing ensures that a function returns correct results for different combinations of its input parameters. These results could be the return value of the function, but they could additionally include output parameters that are passed as pointers or references. For instance, a simple integer multiplication function,
          </li>
         </ol>
        </div>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0035">
           <img alt="image" height="81" src="../../IMAGES/B9780443222191000210/main.assets/u12-03-9780443222191.jpg" width="901"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="textfl">
         <a id="p457">
         </a>
        </p>
        <div>
         <span aria-label="457" epub:type="pagebreak" id="pagebreak_457" role="doc-pagebreak">
         </span>
        </div>
        <p class="text" id="p0370">
         could be tested by supplying a range of (
         <i>
          x
         </i>
         ,
         <i>
          y
         </i>
         ) inputs and checking the results against a table of known correct values.
        </p>
        <div>
         <ol id="ulist0030">
          <li class="bulllist" id="p0375">
           <a id="u0115">
           </a>
           •
           <b>
            Getter/setter pairs
           </b>
           . The use of getter/setter methods is extremely common in C++ APIs, and of course I've advocated that you should always prefer the use of these functions over directly exposing member variables in a class. You should therefore test that calling the getter before calling the setter returns an appropriate default result, and that calling the getter after the setter will return the appropriate value, such as:
          </li>
          <li>
           <div class="pageavoid">
            <figure class="fig" id="f0040">
             <img alt="image" height="197" src="../../IMAGES/B9780443222191000210/main.assets/u12-04-9780443222191.jpg" width="1838"/>
             <figcaption class="figleg">
             </figcaption>
            </figure>
           </div>
          </li>
          <li class="bulllist" id="p0380">
           <a id="u0120">
           </a>
           •
           <b>
            Operation order
           </b>
           . Varying the sequence of operations to perform the same test (where this is possible) can help to uncover order of execution assumptions and nonorthogonal behavior: that is, if the API calls have undocumented side effects that are being relied upon to achieve certain workflows.
          </li>
          <li class="bulllist" id="p0385">
           <a id="u0125">
           </a>
           •
           <b>
            Regression testing
           </b>
           . Backward compatibility with earlier versions of the API should be maintained whenever possible. It's therefore extremely valuable to have tests that verify this goal. For example, a test could try reading data files that were generated by older versions of the API to ensure that the latest version can still ingest them correctly. It's important that these data files are never updated to newer formats when the API is modified. That is, you will end up with live data files, which are up-to-date for the current version, and legacy data files, which verify the backward compatibility of the API.
          </li>
          <li class="bulllist" id="p0390">
           <a id="u0130">
           </a>
           •
           <b>
            Negative testing
           </b>
           . This testing technique constructs or forces error conditions to see how the code reacts to unexpected situations. For example, if an API call attempts to read a file on disk, a negative test might try deleting that file or making it unreadable, to see how the API reacts when it's unable to read the contents of the file. Another example of negative testing is supplying invalid data for an API call. For example, a credit card payment system that accepts credit card numbers should be tested with invalid credit card numbers (negative testing) as well as valid numbers (positive testing).
          </li>
          <li class="bulllist" id="p0395">
           <a id="u0135">
           </a>
           •
           <b>
            Buffer overruns
           </b>
           . A buffer overrun, or overflow, is when memory is written past the end of an allocated buffer. This causes unallocated memory to be modified, often resulting in data corruption and ultimately a crash. Data corruption errors can be difficult to track down because the crash may occur sometime after the actual buffer overrun event. It's therefore good practice to check that an API does not write to memory beyond the size of a buffer. This buffer could be an internal private member of a class or it could be a parameter that you pass into an API call. For example, the
           <span class="inlinecode">
            string.h
           </span>
           function
           <span class="inlinecode">
            strncpy()
           </span>
           copies at most
           <i>
            n
           </i>
           characters from one string to another. This could be tested by supplying source strings that are equal to
           <a id="p458">
           </a>
           <span aria-label="458" epub:type="pagebreak" id="pagebreak_458" role="doc-pagebreak">
           </span>
           and longer than
           <i>
            n
           </i>
           characters, then verifying that no more than
           <i>
            n
           </i>
           characters (including the null terminator,
           <span class="inlinecode">
            ‘\0’
           </span>
           ) are written to the destination buffer.
          </li>
          <li class="bulllist" id="p0400">
           <a id="u0140">
           </a>
           •
           <b>
            Memory ownership
           </b>
           . Memory errors are a common cause of crashes in C++ programs. Any API calls that return dynamically allocated memory should document whether the API owns the memory or if the client is responsible for freeing it. These specifications should be tested to ensure they are correct. For example, if the client is responsible for freeing the memory, a test could request the dynamic object twice and assert that the two pointers are different. A further test could free the memory and then rerequest the object from the API multiple times to ensure that no memory corruption or crashes occur.
          </li>
          <li class="bulllist" id="p0405">
           <a id="u0145">
           </a>
           •
           <b>
            Null input
           </b>
           . Another common source of crashes in C++ is passing a
           <span class="inlinecode">
            nullptr
           </span>
           to a function that then immediately attempts to dereference the pointer without checking for null. You should therefore test all functions that accept a pointer parameter to ensure that they behave gracefully when passed a
           <span class="inlinecode">
            nullptr
           </span>
           .
          </li>
         </ol>
        </div>
       </section>
       <section>
        <a id="s0050">
        </a>
        <h3 class="h2hd" id="cesectitle0060">
         Focusing the testing effort
        </h3>
        <p class="textfl" id="p0410">
         In all likelihood, it will be infeasible to test every possible code path in your API. You will therefore be faced with a decision about which subset of the overall functionality to test. To help you focus your testing effort, this list enumerates seven ways to determine the biggest bang for your testing buck:
        </p>
        <div>
         <ul class="ce_list" id="olist0020">
          <li class="numlist" id="p0415">
           <a id="o0030">
           </a>
           1. Focus on tests that exercise the primary use cases or workflows of the API.
          </li>
          <li class="numlist" id="p0420">
           <a id="o0035">
           </a>
           2. Focus on tests that cover multiple features or offer the widest code coverage.
          </li>
          <li class="numlist" id="p0425">
           <a id="o0040">
           </a>
           3. Focus on the code that is the most complex and hence the highest risk.
          </li>
          <li class="numlist" id="p0430">
           <a id="o0045">
           </a>
           4. Focus on parts of the design that are poorly defined.
          </li>
          <li class="numlist" id="p0435">
           <a id="o0050">
           </a>
           5. Focus on features with the highest performance or security concerns.
          </li>
          <li class="numlist" id="p0440">
           <a id="o0055">
           </a>
           6. Focus on testing problems that would cause the worst impact on clients.
          </li>
          <li class="numlist" id="p0445">
           <a id="o0060">
           </a>
           7. Focus early testing efforts on features that can be completed early in the development cycle.
          </li>
         </ul>
        </div>
       </section>
       <section>
        <a id="s0055">
        </a>
        <h3 class="h2hd" id="cesectitle0065">
         Working with quality assurance
        </h3>
        <p class="textfl" id="p0450">
         If you're fortunate enough to have a good QA team to support your testing efforts, then they can share responsibility for writing automated tests. For example, it's standard practice for developers to write and own unit tests and for QA to write and own integration tests.
        </p>
        <p class="text" id="p0455">
         Different software development models produce different interactions with QA. For example, a traditional waterfall method, in which testing is performed as a final step before release, means that QA is often treated as a distinct group whose goal of quality is often negatively affected by delays during the development process. By contrast, more agile development processes such as Scrum favor embedding QA as part of the
         <a id="p459">
         </a>
         <span aria-label="459" epub:type="pagebreak" id="pagebreak_459" role="doc-pagebreak">
         </span>
         development process and including testing responsibilities within each short sprint or iteration. In either case, the benefit of working with QA engineers is that they become your first users. As such, they can help to ensure that the functional and business requirements of your API are met.
        </p>
        <p class="text" id="p0460">
         I noted earlier that API testing generally requires writing code, because an API is software that's used to build end user applications. This implies that your QA engineers must be able to write code to work on integration testing effectively. Related to this, Microsoft has traditionally used two broad terms to categorize QA engineers:
        </p>
        <div>
         <ul class="ce_list" id="olist0025">
          <li class="numlist" id="p0465">
           <a id="o0065">
           </a>
           1.
           <b>
            Software test engineer (STE)
           </b>
           : has limited programming experience and may not even need a strong computer science background. An STE essentially performs manual closed box testing.
          </li>
          <li class="numlist" id="p0470">
           <a id="o0070">
           </a>
           2.
           <b>
            Software design engineer in test (SDET)
           </b>
           : can write code and is therefore capable of performing open box testing, writing tools, and producing automated tests.
          </li>
         </ul>
        </div>
        <p class="text" id="p0475">
         In terms of API testing, you will therefore more likely prefer a QA engineer who is an SDET rather than an STE. However, even most SDETs will not be able to program in C++, although most will be able to write code in a scripting language. Providing script bindings for your API can therefore offer greater opportunity for your QA team to contribute automated integration tests (see
         <a href="../B978044322219100012X/CH0014_501-531_B978044322219100012X.xhtml">
          Chapter 14
         </a>
         on Scripting for details on adding scripting support). Another technique is to write programs that enable data-driven testing. The earlier reference to
         <span class="inlinecode">
          pngtest.c
         </span>
         is an example of this: a single program written by a developer that can be used by QA engineers to produce a slew of data-driven integration tests. As noted earlier, your SDK may include one or more command-line utilities, which could be tested using a scripting language such as Python instead of C++.
        </p>
       </section>
      </section>
      <section>
       <a id="s0060">
       </a>
       <h2 class="h1hd" id="cesectitle0070">
        Writing testable code
       </h2>
       <p class="textfl" id="p0480">
        Testing an API shouldn't be something that you leave until the end of the process. There are decisions you make while you're designing and implementing an API that can improve your ability to write robust and extensive automated tests. In other words, you should consider how a class will be tested early on during its development. In the following sections, I will cover various techniques for writing software that's more amenable to automated unit and integration testing.
       </p>
       <section>
        <a id="s0065">
        </a>
        <h3 class="h2hd" id="cesectitle0075">
         Test-driven development
        </h3>
        <p class="textfl" id="p0485">
         Test-driven development (TDD), or test-first programming, involves writing automated tests to verify desired functionality before the code that implements this functionality is written. These tests will, of course, fail initially. The goal is then to write minimal code quickly to make these tests pass. Then finally, the code is refactored to optimize or clean up the implementation as necessary (
         <a href="../B9780443222191160015/References_583-587_B9780443222191160015.xhtml#bib8" id="bib_8">
          Beck, 2002
         </a>
         ).
         <a id="p460">
         </a>
        </p>
        <div>
         <span aria-label="460" epub:type="pagebreak" id="pagebreak_460" role="doc-pagebreak">
         </span>
        </div>
        <p class="text" id="p0490">
         An important aspect of TDD is that changes are made incrementally, in small steps. You write a short test, then write enough code to make that test pass, then repeat. After every small change, you recompile your code and rerun the tests. Working in these small steps means that if a test starts to fail, then in all probability this will be caused by the code you wrote since the last test run. Let's look at an example to demonstrate this. I'll start with a small test to verify the behavior of a
         <span class="inlinecode">
          MovieRating
         </span>
         class (
         <a href="../B9780443222191160015/References_583-587_B9780443222191160015.xhtml#bib5" id="bib_5">
          Astels, 2003
         </a>
         ):
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0045">
           <img alt="image" height="313" src="../../IMAGES/B9780443222191000210/main.assets/u12-05-9780443222191.jpg" width="1977"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0495">
         Given this initial test code, you now write the simplest possible code to make the test pass. Here's an example that satisfies this objective (I will inline the implementation for the API methods in these examples to make it clearer how the code under test evolves):
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0050">
           <img alt="image" height="371" src="../../IMAGES/B9780443222191000210/main.assets/u12-06-9780443222191.jpg" width="1524"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0500">
         This API clearly doesn't do a lot, but it does allow the previous test to pass. So now you can move on and add some more test code:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0055">
           <img alt="image" height="429" src="../../IMAGES/B9780443222191000210/main.assets/u12-07-9780443222191.jpg" width="2325"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0505">
         Now it’s time to write the minimal code to make this test pass:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0060">
           <img alt="image" height="487" src="../../IMAGES/B9780443222191000210/main.assets/u12-08-9780443222191.jpg" width="1770"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="textfl">
         <a id="p461">
         </a>
        </p>
        <div>
         <span aria-label="461" epub:type="pagebreak" id="pagebreak_461" role="doc-pagebreak">
         </span>
        </div>
        <p class="text" id="p0510">
         Writing another test will force us to make the implementation more general:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0065">
           <img alt="image" height="545" src="../../IMAGES/B9780443222191000210/main.assets/u12-09-9780443222191.jpg" width="2325"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0515">
         Now you should extend the implementation to return the number of ratings added and the average of those ratings. The minimal way to do this would be to record the current sum of all ratings and the number of ratings added. For example:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0070">
           <img alt="image" height="1702" src="../../IMAGES/B9780443222191000210/main.assets/u12-10-9780443222191.jpg" width="1492"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0520">
         Obviously you can continue this strategy by adding further tests to verify that calling
         <span class="inlinecode">
          GetAverageRating()
         </span>
         with zero ratings does not crash and to check that adding out-of-range rating values is treated appropriately, but I think you get the general principle.
        </p>
        <p class="text" id="p0525">
         One of the main benefits of TDD is that it forces you to think about your API before you start writing any code. You also must think about how the API will be used (i.e., you put yourself in the shoes of your clients). Another effect of TDD is that you implement
         <a id="p462">
         </a>
         <span aria-label="462" epub:type="pagebreak" id="pagebreak_462" role="doc-pagebreak">
         </span>
         only what your tests need. In other words, your tests determine the code you need to write (
         <a href="../B9780443222191160015/References_583-587_B9780443222191160015.xhtml#bib5">
          Astels, 2003
         </a>
         ). This can help you avoid premature optimization and keeps you focused on the overall behavior.
        </p>
        <div>
         <aside aria-labelledby="b0040" epub:type="sidebar">
          <div class="box_top_space">
          </div>
          <div class="boxg1" id="b0040">
           <div class="b1textfl" id="bpar0125k">
            <i>
             TIP: TDD means that you write unit tests first, then write the code to make the tests pass. This keeps you focused on the key use cases for your API.
            </i>
           </div>
          </div>
         </aside>
        </div>
        <p class="text" id="p0535">
         TDD doesn't have to be confined to the initial development of your API. It can also be helpful during maintenance mode. For example, when a bug is discovered in your API, you should first write a test for the correct behavior. This test will, of course, fail at first. You can then work on implementing the fix for the bug. You will know when the bug is fixed because your test will change to a pass state. Once the bug is fixed, you then have the added benefit of an ongoing regression test that will ensure the same bug is not introduced again in the future.
        </p>
       </section>
       <section>
        <a id="s0070">
        </a>
        <h3 class="h2hd" id="cesectitle0080">
         Stub and mock objects
        </h3>
        <p class="textfl" id="p0540">
         One popular technique to make your unit tests more stable and resilient to failures is to create test objects that can stand in for real objects in the system. This lets you substitute an unpredictable resource with a lightweight controllable replacement for the purpose of testing. Examples of unpredictable resources include the file system, external databases, and networks. The stand-in object can also be used to test error conditions that are difficult to simulate in the real system, as well as events that are triggered at a certain time or are based upon a random number generator.
        </p>
        <p class="text" id="p0545">
         These stand-in objects will obviously present the same interface as the real objects they simulate. However, there are several different ways to implement these objects. This list presents some of the options and introduces the generally accepted terminology for each case:
        </p>
        <div>
         <ol id="ulist0040">
          <li class="bulllist" id="p0550">
           <a id="u0150">
           </a>
           ▪
           <b>
            Fake Object
           </b>
           : An object that has functional behavior but uses a simpler implementation to aid testing: for example, an in-memory file system that simulates interactions with the local disk.
          </li>
          <li class="bulllist" id="p0555">
           <a id="u0155">
           </a>
           ▪
           <b>
            Stub Object
           </b>
           : An object that returns prepared or canned responses: for example, a
           <span class="inlinecode">
            ReadFileAsString()
           </span>
           stub might simply return a hardcoded string as the file contents, rather than reading the contents of the named file on disk.
          </li>
          <li class="bulllist" id="p0560">
           <a id="u0160">
           </a>
           ▪
           <b>
            Mock Object
           </b>
           : An instrumented object that has a preprogrammed behavior and that performs verification on the calling sequence of its methods: for example, a mock object (or simply a mock) can specify that a
           <span class="inlinecode">
            GetValue()
           </span>
           function will return 10 the first two times it's called, then 20 after that. It can also verify that the function was called, say, only three times, or at least five times, or that the functions in the class were called in a given order.
          </li>
         </ol>
        </div>
        <p class="textfl">
         <a id="p463">
         </a>
        </p>
        <div>
         <span aria-label="463" epub:type="pagebreak" id="pagebreak_463" role="doc-pagebreak">
         </span>
        </div>
        <p class="text" id="p0565">
         The difference between a stub and a mock is often poorly understood, so let's demonstrate this with an example using the children's card game, War. This is a simple game in which a deck of cards is divided equally between two players. Each player reveals the top card and the player with the highest card takes both cards. If the cards have equal value, each player lays three cards face down and the fourth face up. The highest value card wins all of the cards on the table. A player wins the game by collecting all of the cards.
        </p>
        <p class="text" id="p0570">
         I'll model this game with three classes:
        </p>
        <div>
         <ul class="ce_list" id="olist0030">
          <li class="numlist" id="p0575">
           <a id="o0075">
           </a>
           1.
           <b>
            Card
           </b>
           : represents a single card with the ability to compare its value against another card.
          </li>
          <li class="numlist" id="p0580">
           <a id="o0080">
           </a>
           2.
           <b>
            Deck
           </b>
           : holds a deck of cards with functions to shuffle and deal cards.
          </li>
          <li class="numlist" id="p0585">
           <a id="o0085">
           </a>
           3.
           <b>
            WarGame
           </b>
           : manages the game logic, with functions to play out the entire game and return the winner of the game.
          </li>
         </ul>
        </div>
        <p class="text" id="p0590">
         During actual game play, the
         <span class="inlinecode">
          Deck
         </span>
         object will return a random card. However, for the purposes of testing, you could create a stub deck that returns cards in a predefined order. If the
         <span class="inlinecode">
          WarGame
         </span>
         object accepts the deck to use as a parameter to its constructor, you can easily test the logic of
         <span class="inlinecode">
          WarGame
         </span>
         by passing it a
         <span class="inlinecode">
          StubDeck
         </span>
         that defines a specific and repeatable sequence of cards.
        </p>
        <p class="text" id="p0595">
         This
         <span class="inlinecode">
          StubDeck
         </span>
         would inherit from the real Deck class, which means that you must design
         <span class="inlinecode">
          Deck
         </span>
         to be a base class (i.e., make the destructor virtual as well as any methods that need to be overridden for testing purposes). Here's an example declaration for the
         <span class="inlinecode">
          Deck
         </span>
         class:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0075">
           <img alt="image" height="545" src="../../IMAGES/B9780443222191000210/main.assets/u12-11-9780443222191.jpg" width="1144"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0600">
         Our
         <span class="inlinecode">
          StubDeck
         </span>
         class can therefore inherit from
         <span class="inlinecode">
          Deck
         </span>
         and override the
         <span class="inlinecode">
          Shuffle()
         </span>
         method to do nothing, because you don't want to randomize the card order. Then the constructor of
         <span class="inlinecode">
          StubDeck
         </span>
         could create a specific order of cards. However, this means that the stub class is hardcoded to a single card order. A more general solution would be to extend the class with an
         <span class="inlinecode">
          AddCard()
         </span>
         method. Then you can write multiple tests using
         <span class="inlinecode">
          StubDeck
         </span>
         and simply call
         <span class="inlinecode">
          AddCard()
         </span>
         a number of times to prepare it with a specific order of cards before passing it to
         <span class="inlinecode">
          WarGame
         </span>
         . One way to do this would be to add a protected
         <span class="inlinecode">
          AddCard()
         </span>
         method to the base Deck class (because it modifies private state) and then expose this as public in the
         <span class="inlinecode">
          StubDeck
         </span>
         class. Then you can write:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0080">
           <img alt="image" height="833" src="../../IMAGES/B9780443222191000210/main.assets/u12-12-9780443222191.jpg" width="1906"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="textfl">
         <a id="p464">
         </a>
        </p>
        <div>
         <span aria-label="464" epub:type="pagebreak" id="pagebreak_464" role="doc-pagebreak">
         </span>
        </div>
        <p class="text" id="p0605">
         So, that's what a stub object would look like (in fact, this could even be considered a fake object, too, because it offers complete functionality but without the element of randomness). Let's now look at what testing with a mock object looks like.
        </p>
        <p class="text" id="p0610">
         One of the main differences between mock and stub objects is that mocks insist on behavior verification. That is, a mock object is instrumented to record all function calls for an object and it will verify behavior such as the number of times a function was called, the parameters that were passed to the function, or the order in which several functions were called. Writing code to perform this instrumentation by hand can be tedious and error prone. It's therefore best to rely upon a mock testing framework to automate this work for you. I'll use the Google Mock framework here (
         <a href="https://github.com/google/googletest">
          https://github.com/google/googletest
         </a>
         ) to illustrate how mocks can be used to test our
         <span class="inlinecode">
          WarGame
         </span>
         class. The first thing you'll want to do is define the mock using the handy macros that Google Mock provides:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0085">
           <img alt="image" height="776" src="../../IMAGES/B9780443222191000210/main.assets/u12-13-9780443222191.jpg" width="1386"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0615">
         The
         <span class="inlinecode">
          MOCK_METHOD0
         </span>
         macro is used to instrument functions with zero arguments, which is the case for all methods in the
         <span class="inlinecode">
          Deck
         </span>
         base class. If instead you have a method with one argument, then you would use
         <span class="inlinecode">
          MOCK_METHOD1
         </span>
         , and so on. Now, let's write a unit test that
         <a id="p465">
         </a>
         <span aria-label="465" epub:type="pagebreak" id="pagebreak_465" role="doc-pagebreak">
         </span>
         uses this mock. Because I'm using Google Mock to create our mock, I'll also use Google Test as the testing framework. This looks like:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0090">
           <img alt="image" height="1123" src="../../IMAGES/B9780443222191000210/main.assets/u12-14-9780443222191.jpg" width="1836"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0620">
         The clever bits are those two
         <span class="inlinecode">
          EXPECT_CALL()
         </span>
         lines. The first one states that the
         <span class="inlinecode">
          Shuffle()
         </span>
         method of our mock object should get called at least once, and the second one states that the
         <span class="inlinecode">
          DealCard()
         </span>
         method should get called exactly 52 times, and that the first call will return
         <span class="inlinecode">
          Card("JS")
         </span>
         , the second call will return
         <span class="inlinecode">
          Card("2H")
         </span>
         , and so on. This approach means that you don't need to expose an
         <span class="inlinecode">
          AddCard()
         </span>
         method for your mock object. The mock object will implicitly verify all of the expectations as part of its destructor and will fail the test if any of these are not met.
        </p>
        <div>
         <aside aria-labelledby="b0044" epub:type="sidebar">
          <div class="box_top_space">
          </div>
          <div class="boxg1" id="b0044">
           <div class="b1textfl" id="bpar0125l">
            <i>
             TIP: Both stub and mock objects can return canned responses, but mock objects also perform call behavior verification.
            </i>
           </div>
          </div>
         </aside>
        </div>
        <p class="text" id="p0630">
         In terms of how this affects the design of your APIs, one implication is that you may wish to consider a model in which access to unpredictable resources is embodied within a base class that you pass into your worker classes, such as in the previous case in which you pass the
         <span class="inlinecode">
          Deck
         </span>
         object into the
         <span class="inlinecode">
          WarGame
         </span>
         object. This allows you to substitute a stub or mock version in your test code using inheritance. This is essentially the dependency injection pattern, in which dependent objects are passed into a class rather than that class being directly responsible for creating and storing those objects.
        </p>
        <p class="text" id="p0635">
         However, sometimes it's simply not practical to encapsulate and pass in all of the external dependencies for a class. In these cases, you can still use stub or mock objects, but rather than using inheritance to replace functionality, you can inject them physically at link time. In this case, you name the stub/mock class the same as the class you wish to replace. Then your test program links against the test code and not the code with the real implementation. Using our
         <span class="inlinecode">
          ReadFileAsString()
         </span>
         example from earlier, you could create an alternate version of this function that returns canned data, then link the object
         <span class="inlinecode">
          .o
         </span>
         file with this stub into our test program in place of the object file that holds the real implementation.
         <a id="p466">
         </a>
         <span aria-label="466" epub:type="pagebreak" id="pagebreak_466" role="doc-pagebreak">
         </span>
         This approach can be powerful, although it necessitates you to create your own abstractions for accessing the file system, network, and so on. If your code directly calls
         <span class="inlinecode">
          fopen()
         </span>
         from the standard library, then you can't replace this with a stub at link time unless you also provide stubs for all other standard library functions that your code calls.
        </p>
       </section>
       <section>
        <a id="s0075">
        </a>
        <h3 class="h2hd" id="cesectitle0085">
         Testing private code
        </h3>
        <p class="textfl" id="p0640">
         The emphasis of this book has been on developing well-designed APIs that offer a logical abstraction while hiding implementation details. However, this can also make it difficult to write thorough unit tests. There will be times when you need to write a unit test that accesses private members of a class to achieve full code coverage. Given a class called
         <span class="inlinecode">
          MyClass
         </span>
         , this can be done in several ways, including:
        </p>
        <div>
         <ul class="ce_list" id="olist0035">
          <li class="numlist" id="p0645">
           <a id="o0090">
           </a>
           1.
           <b>
            Public member function
           </b>
           : Declaring a public
           <span class="inlinecode">
            MyClass::SelfTest()
           </span>
           method for your test code to call.
          </li>
          <li class="numlist" id="p0650">
           <a id="o0095">
           </a>
           2.
           <b>
            Friend function
           </b>
           : Creating a
           <span class="inlinecode">
            MyClassSelfTest()
           </span>
           free function and declaring it as friend function in
           <span class="inlinecode">
            MyClass
           </span>
           .
          </li>
          <li class="numlist" id="p0655">
           <a id="o0100">
           </a>
           3.
           <b>
            Friend class
           </b>
           : Declaring a private
           <span class="inlinecode">
            MyClass::SelfTest()
           </span>
           method and creating a friend
           <span class="inlinecode">
            TestRunner
           </span>
           object that can run the testing function.
          </li>
         </ul>
        </div>
        <p class="text" id="p0660">
         I detailed several reasons to avoid friends in
         <a href="../B9780443222191000106/CH0006_209-250_B9780443222191000106.xhtml">
          Chapter 6
         </a>
         on C++ Usage, although in this case the use of a friend function or class can help you avoid exposing internal functions that are not meant to be used by your clients. For example, the friend function can be made relatively safe if the
         <span class="inlinecode">
          MyClassSelfTest()
         </span>
         function is defined in the same library as the
         <span class="inlinecode">
          MyClass
         </span>
         implementation, thus preventing clients from redefining the function in their own code. The Google Test framework provides a
         <span class="inlinecode">
          FRIEND_TEST()
         </span>
         macro to support this kind of friend function testing.
        </p>
        <p class="text" id="p0665">
         I'll start by covering the public
         <span class="inlinecode">
          SelfTest()
         </span>
         method, and then I'll provide a solution using a friend class. In both cases, I'll work with a simple bounding box class that includes a self-test method.
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0095">
           <img alt="image" height="1354" src="../../IMAGES/B9780443222191000210/main.assets/u12-15-9780443222191.jpg" width="1734"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="textfl">
         <a id="p467">
         </a>
        </p>
        <div>
         <span aria-label="467" epub:type="pagebreak" id="pagebreak_467" role="doc-pagebreak">
         </span>
        </div>
        <p class="text" id="p0670">
         The
         <span class="inlinecode">
          SelfTest()
         </span>
         public method can be called directly from a unit test to perform extra validation of the various private methods. This is convenient for testing, although there are some undesirable qualities of this approach: namely, you have to pollute your public API with a method that your clients should not call, and you may add extra bloat to your library by embedding the test code inside the
         <span class="inlinecode">
          BBox
         </span>
         implementation.
        </p>
        <p class="text" id="p0675">
         In the first case, there are ways that you can discourage clients from using this function. One trivial way to do this would be simply to add a comment that the method is not for public use. Taking this one step further, you could remove the method from any API documentation you produce, so that users never see a reference to it (unless they look directly at your headers, of course). As I discussed in the Versioning chapter, you can achieve this with the Doxygen tool by surrounding the function declaration with the
         <span class="inlinecode">
          \cond
         </span>
         and
         <span class="inlinecode">
          \endcond
         </span>
         commands:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0100">
           <img alt="image" height="197" src="../../IMAGES/B9780443222191000210/main.assets/u12-16-9780443222191.jpg" width="796"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0680">
         As for the concern that the self-test function may add bloat to your code, there are a couple of ways to deal with this, if you feel it's necessary. One way would be to implement the
         <span class="inlinecode">
          SelfTest()
         </span>
         method in your unit test code, not in the main API code (e.g., in
         <span class="inlinecode">
          test_bbox.cpp
         </span>
         not
         <span class="inlinecode">
          bbox.cpp)
         </span>
         . Just because you declare a method in your
         <span class="inlinecode">
          .h
         </span>
         file doesn't mean that you must define it. However, this opens up a similar security hole to using friends. That is, your clients could define the
         <span class="inlinecode">
          SelfTest()
         </span>
         method in their own code as a way to modify the internal state of the object. Although the interface of this function restricts what they can do, because they cannot pass in any arguments or receive any results, they can still use global variables to circumvent this.
        </p>
        <p class="text" id="p0685">
         An alternative would be to compile the test code conditionally. For example,
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0105">
           <img alt="image" height="602" src="../../IMAGES/B9780443222191000210/main.assets/u12-17-9780443222191.jpg" width="2219"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0690">
         The downside of this approach is that you have to build two versions of your API: one with the self-test code compiled in (compiled with
         <span class="inlinecode">
          -DTEST
         </span>
         or
         <span class="inlinecode">
          /DTEST
         </span>
         ) and one without the self-test code. If the extra build is a problem, you could compile the self-test code into debug versions of your library but remove it from release builds.
         <a id="p468">
         </a>
        </p>
        <div>
         <span aria-label="468" epub:type="pagebreak" id="pagebreak_468" role="doc-pagebreak">
         </span>
        </div>
        <p class="text" id="p0695">
         If, however, you really don't want to expose your
         <span class="inlinecode">
          SelfTest()
         </span>
         function publicly, then you can use friends to help. For example, we can introduce a
         <span class="inlinecode">
          TestRunner
         </span>
         class that's responsible for running the
         <span class="inlinecode">
          SelfTest()
         </span>
         function, in which the object under test can make its testing function private and declare
         <span class="inlinecode">
          TestRunner
         </span>
         as a friend. We can make this more reusable by introducing a
         <span class="inlinecode">
          Testable
         </span>
         abstract interface from which objects can inherit if they want to provide a
         <span class="inlinecode">
          SelfTest()
         </span>
         function for the
         <span class="inlinecode">
          TestRunner
         </span>
         to run. For example:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0110">
           <img alt="image" height="1528" src="../../IMAGES/B9780443222191000210/main.assets/u12-18-9780443222191.jpg" width="1561"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0700">
         Now you can easily add a self-testing function to any object by inheriting from the
         <span class="inlinecode">
          Testable
         </span>
         interface and defining your private
         <span class="inlinecode">
          SelfTest()
         </span>
         function, all without exposing your testing code to your clients. For example:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0115">
           <img alt="image" height="197" src="../../IMAGES/B9780443222191000210/main.assets/u12-19-9780443222191.jpg" width="2151"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <div>
         <aside aria-labelledby="b0095" epub:type="sidebar">
          <div class="box_top_space">
          </div>
          <div class="boxg1" id="b0095">
           <div class="b1textfl" id="bpar0125o">
            <i>
             TIP: Use a
            </i>
            <span class="inlinecode">
             <i>
              SelfTest()
             </i>
            </span>
            <i>
             member function to test private members of a class.
            </i>
           </div>
          </div>
         </aside>
        </div>
        <p class="text" id="p0710">
         If you wish to provide a self-test function for a C API, then this is a much simpler proposition. For example, you could define an external linkage
         <span class="inlinecode">
          SelfTest()
         </span>
         function in the
         <span class="inlinecode">
          .c
         </span>
         file, that is, a nonstatic function that is decorated with
         <span class="inlinecode">
          __declspec(dllexport)
         </span>
         on Windows, but provide no prototype for the function in the
         <span class="inlinecode">
          .h
         </span>
         file. You then declare the
         <a id="p469">
         </a>
         <span aria-label="469" epub:type="pagebreak" id="pagebreak_469" role="doc-pagebreak">
         </span>
         function prototype in your test code so that you can call the function as part of your unit test. In this way, the function does not appear in your header file or any API documentation. In fact, the only way clients could discover the call is if they do a dump of all of the public symbols in your shared library.
        </p>
       </section>
       <section>
        <a id="s0080">
        </a>
        <h3 class="h2hd" id="cesectitle0090">
         Using assertions
        </h3>
        <p class="textfl" id="p0715">
         An assertion is a way to verify assumptions that your code makes. You do this by encoding the assumption in a call to an assert function or macro. If the value of the expression evaluates to true, then all is well and nothing happens. However, if the expression evaluates to false, then the assumption you made in the code is invalid and your program will abort with an appropriate error (
         <a href="../B9780443222191160015/References_583-587_B9780443222191160015.xhtml#bib56">
          McConnell, 2004
         </a>
         ).
        </p>
        <p class="text" id="p0720">
         Assertions are essentially a way for you to include extra sanity tests for your program state directly in the code. As such, these are invaluable complementary aids to help testing and debugging.
        </p>
        <p class="text" id="p0725">
         Although you are free to write your own assertion routines, the C standard library includes an
         <span class="inlinecode">
          assert()
         </span>
         macro in the
         <span class="inlinecode">
          assert.h
         </span>
         header (also available in C++ as the
         <span class="inlinecode">
          cassert
         </span>
         header). The next example uses this macro to show how you could document and enforce the assumption that a pointer you are about to dereference is nonnull:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0120">
           <img alt="image" height="834" src="../../IMAGES/B9780443222191000210/main.assets/u12-20-9780443222191.jpg" width="2080"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0730">
         It is common practice to turn off all
         <span class="inlinecode">
          assert()
         </span>
         calls for production code, so that an end user application doesn't abort needlessly when the user is running it. This is often done by making assert calls do nothing when they're compiled in release mode versus debug mode. (Thus you should never put code that must always be executed into an assertion.) Here's an example of a simple assertion definition that is active only in debug builds:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0125">
           <img alt="image" height="313" src="../../IMAGES/B9780443222191000210/main.assets/u12-21-9780443222191.jpg" width="692"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0735">
         You can also compile with the
         <span class="inlinecode">
          NDEBUG
         </span>
         define to disable assertions in
         <span class="inlinecode">
          assert.h
         </span>
         .
         <a id="p470">
         </a>
        </p>
        <div>
         <span aria-label="470" epub:type="pagebreak" id="pagebreak_470" role="doc-pagebreak">
         </span>
        </div>
        <p class="text" id="p0740">
         Assertions should be used to document conditions that you as the developer believe should never occur. They're not appropriate for run-time error conditions that might legitimately occur. If you can recover gracefully from an error, then you should always prefer that course of action rather than causing the client's program to crash. For example, if you have an API call that accepts a pointer from the client, you should never assume that it is nonnull. Instead, you should check for
         <span class="inlinecode">
          nullptr
         </span>
         and return gracefully if that's the case, potentially emitting an appropriate error message. You should not use an assertion for this case. However, if your API enforces the condition that one of your private member variables is always nonnull, then it would be a programming error for it ever to be null. This is an appropriate situation for an assertion. Use assertions to check for programming errors; use normal error checking to test for user errors and attempt to recover gracefully in that situation.
        </p>
        <div>
         <aside aria-labelledby="b0050" epub:type="sidebar">
          <div class="box_top_space">
          </div>
          <div class="boxg1" id="b0050">
           <div class="b1textfl" id="bpar0125c">
            <i>
             TIP: Use assertions to document and verify programming errors that should never occur.
            </i>
           </div>
          </div>
         </aside>
        </div>
        <p class="text" id="p0750">
         Assertions are commonly used in commercial products to diagnose errors. For instance, several years ago it was reported that the Microsoft Office suite is covered by over 250,000 assertions (
         <a href="../B9780443222191160015/References_583-587_B9780443222191160015.xhtml#bib40" id="bib_40">
          Hoare, 2003
         </a>
         ). These are often used in conjunction with other automated testing techniques, such as running a large suite of unit and integration test cases on debug code with the assertions turned on. This test run will fail if any test code hits an assertion that fails, allowing a developer to follow up and investigate the reason for the failure before it leads to a crash in client code.
        </p>
        <p class="text" id="p0755">
         C++11 introduced support for compile-time assertions with
         <span class="inlinecode">
          static_assert()
         </span>
         . If the constant expression for this assertion resolves to false at compile time, then the compiler displays the provided error message and fails; otherwise, the statement has no effect. For example,
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0130">
           <img alt="image" height="197" src="../../IMAGES/B9780443222191000210/main.assets/u12-22-9780443222191.jpg" width="2081"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
       </section>
       <section>
        <a id="s0085">
        </a>
        <h3 class="h2hd" id="cesectitle0095">
         Contract programming
        </h3>
        <p class="textfl" id="p0760">
         Bertrand Meyer coined and trademarked the term design by contract to prescribe the obligations between an interface and its clients (
         <a href="../B9780443222191160015/References_583-587_B9780443222191160015.xhtml#bib65" id="bib_65">
          Meyer, 1987
         </a>
         ). For function calls, this means specifying the preconditions that a client must meet before calling the function, and the postconditions that the function guarantees on exit. For classes, this means defining the invariants that it maintains before and after a public method call (
         <a href="../B9780443222191160015/References_583-587_B9780443222191160015.xhtml#bib39" id="bib_39">
          Hoare, 1969
         </a>
         ;
         <a href="../B9780443222191160015/References_583-587_B9780443222191160015.xhtml#bib79" id="bib_79">
          Pugh, 2006
         </a>
         ).
        </p>
        <p class="text" id="p0765">
         In the previous Documentation chapter, I showed you how to communicate these conditions and constraints to your users via your API documentation. Here I'll illustrate
         <a id="p471">
         </a>
         <span aria-label="471" epub:type="pagebreak" id="pagebreak_471" role="doc-pagebreak">
         </span>
         how you can also implement them in code using assertion-style checks. For instance, continuing with the
         <span class="inlinecode">
          SquareRoot()
         </span>
         function I introduced earlier, this code shows how to implement tests for its precondition and postcondition:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0135">
           <img alt="image" height="718" src="../../IMAGES/B9780443222191000210/main.assets/u12-23-9780443222191.jpg" width="1804"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0770">
         The
         <span class="inlinecode">
          require()
         </span>
         and
         <span class="inlinecode">
          ensure()
         </span>
         calls in this example can be implemented in a similar fashion to the
         <span class="inlinecode">
          assert()
         </span>
         macro I described in the previous section (i.e., they do nothing if the condition evaluates to true, otherwise they abort or throw an exception). Just as in the use of assertions, it's common to disable these calls for release builds to avoid their overhead in a production environment and to avoid aborting your clients' programs. In other words, you could simply define these functions as:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0140">
           <img alt="image" height="313" src="../../IMAGES/B9780443222191000210/main.assets/u12-24-9780443222191.jpg" width="1420"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0775">
         Furthermore, you may implement a private method for your classes to test its invariants (i.e., that it's in a valid state). You can then call this method from inside your functions to ensure that the object is in a valid state when the function begins and ends. If you use a consistent name for this method (which you could enforce through the use of an abstract base class), then you could augment your
         <span class="inlinecode">
          require()
         </span>
         and
         <span class="inlinecode">
          ensure()
         </span>
         macros with a
         <span class="inlinecode">
          check_invariants()
         </span>
         macro as:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0145">
           <img alt="image" height="660" src="../../IMAGES/B9780443222191000210/main.assets/u12-25-9780443222191.jpg" width="2082"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="textfl">
         <a id="p472">
         </a>
        </p>
        <div>
         <span aria-label="472" epub:type="pagebreak" id="pagebreak_472" role="doc-pagebreak">
         </span>
        </div>
        <p class="text" id="p0780">
         Putting all of this together, here is a further example of contract programming for a string append method:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0150">
           <img alt="image" height="1065" src="../../IMAGES/B9780443222191000210/main.assets/u12-26-9780443222191.jpg" width="2253"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0785">
         When Meyer originally conceived contract programming, he added explicit support for this technique in his Eiffel language. He also used an assertion model to implement this support, as I have done here. However, in Eiffel these assertions would get automatically extracted into the documentation for the class. C++ does not have this innate capability, so you must manually ensure that the assertions in your implementation match the documentation for your interface.
        </p>
        <p class="text" id="p0790">
         Nevertheless, one of the benefits of employing this kind of contract programming is that errors get flagged much closer to the actual source of the problem. This can make a huge difference when trying to debug a complex program, because often the source of an error and the point where it causes a problem are far apart. This is, of course, a general benefit of using assertions.
        </p>
        <div>
         <aside aria-labelledby="b0055" epub:type="sidebar">
          <div class="box_top_space">
          </div>
          <div class="boxg1" id="b0055">
           <div class="b1textfl" id="bpar0125v">
            <i>
             TIP: Enforcing an interface's contract implies the systematic use of assertions, such as
            </i>
            <span class="inlinecode">
             <i>
              require()
             </i>
            </span>
            <i>
             ,
            </i>
            <span class="inlinecode">
             <i>
              ensure()
             </i>
            </span>
            <i>
             , and
            </i>
            <span class="inlinecode">
             <i>
              check_invariants()
             </i>
            </span>
            .
           </div>
          </div>
         </aside>
        </div>
        <p class="text" id="p0800">
         One particularly important piece of advice to remember when employing this programming style is to test against the interface, not the implementation. That is, your precondition and postcondition checks should make sense at the abstraction level of your API. They should not depend upon the specifics of your implementation; otherwise you will find that you have to change the contract whenever you change the implementation.
        </p>
        <div>
         <aside aria-labelledby="b0060" epub:type="sidebar">
          <div class="box_top_space">
          </div>
          <div class="boxg1" id="b0060">
           <div class="b1textfl" id="bpar0125f">
            <i>
             TIP: Perform contract checks against the interface, not the implementation.
            </i>
           </div>
          </div>
         </aside>
        </div>
        <p class="textfl">
         <a id="p473">
         </a>
        </p>
        <div>
         <span aria-label="473" epub:type="pagebreak" id="pagebreak_473" role="doc-pagebreak">
         </span>
        </div>
        <p class="text" id="p0810">
         There are also some design patterns that you can take advantage of to enforce preconditions and postconditions for your API. One such approach is to use the Thread-Safe Interface design pattern, which I covered in the chapter on Concurrency. Using this pattern, a public function can be implemented that performs the necessary preconditions and postconditions and then calls out to a separate protected virtual method to perform the actual work. Clients can then override the virtual method to perform different work, but the public API call will still enforce the same preconditions and postconditions for the client's derived class. For example:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0155">
           <img alt="image" height="1413" src="../../IMAGES/B9780443222191000210/main.assets/u12-27-9780443222191.jpg" width="2012"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0815">
         When thinking about the problem of reliably performing some logic before and after a block of code, you may be tempted to think about applying RAII principles (i.e., create an object in which the precondition is defined in the constructor and the postcondition in the destructor). However, destructors cannot return a result and should not trigger an exception, so RAII may not be a good tool to use in this case.
        </p>
        <p class="text" id="p0820">
         Finally, because this chapter is focused on testing, it's worth noting that most unit testing frameworks allow you to specify
         <span class="inlinecode">
          setup()
         </span>
         and
         <span class="inlinecode">
          teardown()
         </span>
         functions that will be called before and after every unit test within a class. This provides a great way to enforce preconditions and postconditions within your test code. I provide an example of this later in the Automated Testing Tools section.
        </p>
       </section>
       <section>
        <a id="s0090">
        </a>
        <h3 class="h2hd" id="cesectitle0100">
         Record and playback functionality
        </h3>
        <p class="textfl" id="p0825">
         One feature that can be invaluable for testing (and many other tasks) is the ability to record the sequence of calls made to an API and then play them back again at will. Record and playback tools are common in the arena of application or GUI testing, in which user interactions such as button presses and keystrokes are captured and then
         <a id="p474">
         </a>
         <span aria-label="474" epub:type="pagebreak" id="pagebreak_474" role="doc-pagebreak">
         </span>
         played back to repeat the user's actions. However, the same principles can be applied to API testing. This involves instrumenting every function call in your API to be able to log its name, parameters, and return value. Then a playback module can be written that accepts this log, or journal, file and calls each function in sequence. This module can then check that the latest return values match the previously recorded responses.
        </p>
        <p class="text" id="p0830">
         Ordinarily this functionality will be turned off by default, so that the overhead of creating the journal file does not affect the performance of the API. However, it can be switched on in a production environment to capture actual end user activity. These journal files can then be added to your test suite as data-driven integration tests or they can be played back in a debugger to help isolate problems. You can even use them to refine the behavior of your API based upon real-world use information, such as detecting common invalid inputs and adding better error handling for these cases. Your clients could even expose this functionality in their applications to allow their end users to record their actions and play them back themselves, (i.e., to automate repetitive tasks in the application). This is often called a macro capability in end user applications.
        </p>
        <p class="text" id="p0835">
         There are several different ways that you could instrument your API in this fashion. One of the cleaner ways to do this is to introduce a proxy API that essentially forwards straight through to your main API, but which also manages all of the function call logging. In this way, you don't need to pollute your actual API calls with these details, and you always have the option of shipping a vanilla API without any logging functionality. This is demonstrated in this simple example:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0160">
           <img alt="image" height="602" src="../../IMAGES/B9780443222191000210/main.assets/u12-28-9780443222191.jpg" width="1594"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0840">
         Of course, if you already have a wrapper API such as a script binding or a convenience API, then you can simply reuse that interface layer.
        </p>
        <p class="text" id="p0845">
         Gerard Meszaros notes that on their face, record and playback techniques may appear to be counter to agile methodologies such as test-first development. However, he points out that it’s possible to use record and playback in conjunction with test-first methodologies if the journal is stored in a human-readable file format such as XML (
         <a href="../B9780443222191160015/References_583-587_B9780443222191160015.xhtml#bib64" id="bib_64">
          Meszaros, 2003
         </a>
         ). When this is the case, the record and playback infrastructure can be built early on and then tests can be written as data files rather than in code. This has the additional benefit that more junior QA engineers could also contribute data-driven integration tests to the test suite.
         <a id="p475">
         </a>
        </p>
        <div>
         <span aria-label="475" epub:type="pagebreak" id="pagebreak_475" role="doc-pagebreak">
         </span>
        </div>
        <p class="text" id="p0850">
         Adding robust record and playback functionality to your API can be a significant undertaking, but the costs are normally worth it when you consider the benefits of faster test automation and the ability to let your clients easily capture reproduction cases for bug reports.
        </p>
       </section>
       <section>
        <a id="s0095">
        </a>
        <h3 class="h2hd" id="cesectitle0105">
         Supporting internationalization
        </h3>
        <p class="textfl" id="p0855">
         Internationalization (i18n) is the process of enabling a software product to support different languages and regional variations. The related term localization (l10n) refers to the activity of using the underlying internationalization support to provide translations of application text into a specific language and to define the locale settings for a specific region, such as the date format or currency symbol.
        </p>
        <p class="text" id="p0860">
         Internationalization testing can be used to ensure that a product fully supports a given locale or language. This tends to be an activity limited to end user application testing: that is, testing that an application's menus and messages appear in the user's preferred language. However, design decisions that you make during the development of your API can have an impact on how easily your clients can provide localization support in their applications.
        </p>
        <p class="text" id="p0865">
         For example, you may prefer to return integer error codes rather than error messages in a single language. If you do return error messages, then it would be helpful to define all potential error messages in an appropriate header file that your clients can access, so that they can be appropriately localized. Also, you should avoid returning dates or formatted numbers as strings, because these are interpreted differently across locales. For example, 100,000.00 is a valid number in the United States and the United Kingdom, but in France the same number would be formatted as 100 000,00 or 100.000,00.
        </p>
        <p class="text" id="p0870">
         There are several libraries that provide internationalization and localization functionality. You could use one of these libraries to return localized strings to your clients and let them specify the preferred locale for the strings that your API returns. These libraries are often easy to use. For example, the GNU gettext library provides a
         <span class="inlinecode">
          gettext()
         </span>
         function to look up the translation for a string and return it in the language for the current locale (assuming that a translation has been provided). Often, this
         <span class="inlinecode">
          gettext()
         </span>
         function is aliased to
         <span class="inlinecode">
          _
         </span>
         , so that you can write simple code like:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0165">
           <img alt="image" height="81" src="../../IMAGES/B9780443222191000210/main.assets/u12-29-9780443222191.jpg" width="1594"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="text" id="p0875">
         Similarly, the Qt library provides excellent internationalization and localization features. All
         <span class="inlinecode">
          QObject
         </span>
         subclasses that use the
         <span class="inlinecode">
          Q_OBJECT
         </span>
         macro have a
         <span class="inlinecode">
          tr()
         </span>
         member function that behaves similarly to GNU's
         <span class="inlinecode">
          gettext()
         </span>
         function, such as:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0170">
           <img alt="image" height="81" src="../../IMAGES/B9780443222191000210/main.assets/u12-30-9780443222191.jpg" width="1490"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <p class="textfl">
         <a id="p476">
         </a>
        </p>
        <div>
         <span aria-label="476" epub:type="pagebreak" id="pagebreak_476" role="doc-pagebreak">
         </span>
        </div>
       </section>
      </section>
      <section>
       <a id="s0100">
       </a>
       <h2 class="h1hd" id="cesectitle0110">
        Automated testing tools
       </h2>
       <p class="textfl" id="p0880">
        In this section, I'll look at some tools that you can use to support your automated testing efforts. I will divide these into four broad categories:
       </p>
       <div>
        <ul class="ce_list" id="olist0040">
         <li class="numlist" id="p0885">
          <a id="o0105">
          </a>
          1.
          <b>
           Test harnesses
          </b>
          . Software libraries and programs that make it easier to maintain, run, and report results for automated tests.
         </li>
         <li class="numlist" id="p0890">
          <a id="o0110">
          </a>
          2.
          <b>
           Code coverage
          </b>
          . Tools that instrument your code to track the actual statements or branches that your tests executed.
         </li>
         <li class="numlist" id="p0895">
          <a id="o0115">
          </a>
          3.
          <b>
           Bug tracking
          </b>
          . A database-driven application that allows defect reports and feature requests to be submitted, prioritized, assigned, and resolved for your software.
         </li>
         <li class="numlist" id="p0900">
          <a id="o0120">
          </a>
          4.
          <b>
           Continuous build systems
          </b>
          . A system that rebuilds your software and reruns your automated tests whenever a new change is added.
         </li>
        </ul>
       </div>
       <section>
        <a id="s0105">
        </a>
        <h3 class="h2hd" id="cesectitle0115">
         Test harnesses
        </h3>
        <p class="textfl" id="p0905">
         There are many unit test frameworks available for C and C++. Most of these follow a design similar to the classic JUnit framework and provide support for features such as assertion-based testing, fixture setup, grouping of fixtures for multiple tests, and mock objects. In addition to being able to define a single test, a good test framework should provide a way to run an entire suite of tests at once and report the total number of failures.
        </p>
        <p class="text" id="p0910">
         I'll not attempt to describe all available test harnesses here; a Web search on C++ test frameworks will turn up many tools for you to investigate if that is your desire. However, I will provide details for a few of the more popular or interesting frameworks.
        </p>
        <div>
         <ol id="ulist0045">
          <li class="bulllist" id="p0915">
           <a id="u0165">
           </a>
           ▪
           <b>
            CppUnit
           </b>
           (
           <a href="http://cppunit.sourceforge.net/">
            http://cppunit.sourceforge.net/
           </a>
           ): A port of JUnit to C++ originally created by Michael Feathers. This framework supports various helper macros to simplify the declaration of tests, capturing exceptions and a range of output formats including an XML format and a compiler-like output to ease integration with an IDE. CppUnit also provides a few different test runners, including Qt- and MFC-based GUI runners. Version 1 of CppUnit has reached a stable state, and future development is being directed toward CppUnit 2. Michael Feathers has also created an extremely lightweight alternative version of CppUnit called CppUnitLite. Here is a sample test case written using CppUnit, based upon an example from the CppUnit cookbook:
          </li>
         </ol>
        </div>
        <p class="textfl">
         <a id="p477">
         </a>
         <span aria-label="477" epub:type="pagebreak" id="pagebreak_477" role="doc-pagebreak">
         </span>
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0175">
           <img alt="image" height="1933" src="../../IMAGES/B9780443222191000210/main.assets/u12-31-9780443222191.jpg" width="1836"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <div>
         <ol id="ulist0050">
          <li class="bulllist" id="p0920">
           <a id="u0170">
           </a>
           ▪
           <b>
            Boost Test
           </b>
           (
           <a href="http://www.boost.org/">
            http://www.boost.org/
           </a>
           ): Boost includes a Test library for writing test programs, organizing tests into simple test cases and test suites, and controlling their run-time execution. A core value of this library is portability. As such, it uses a conservative subset of C++ features and minimizes dependencies on other APIs. This has allowed the library to be used for porting and testing of other Boost libraries. Boost Test provides an execution monitor that can catch exceptions in test code, as well as a program execution monitor that can check for exceptions and nonzero return codes from an end user application. The next example, derived from the Boost Test manual, demonstrates how to write a simple unit test using this library:
          </li>
         </ol>
        </div>
        <p class="textfl">
         <a id="p478">
         </a>
         <span aria-label="478" epub:type="pagebreak" id="pagebreak_478" role="doc-pagebreak">
         </span>
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0180">
           <img alt="image" height="2157" src="../../IMAGES/B9780443222191000210/main.assets/u12-32-9780443222191.jpg" width="1850"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <div>
         <ol id="ulist0055">
          <li class="bulllist" id="p0925">
           <a id="u0175">
           </a>
           ▪
           <b>
            Google Test
           </b>
           (
           <a href="https://github.com/google/googletest">
            https://github.com/google/googletest
           </a>
           ): The Google C++ Testing Framework provides a JUnit-style unit test framework for C++. It's a cross-platform system that supports automatic test discovery (i.e., you don't have to enumerate all of the tests in your test suite manually) and a rich set of assertions, including fatal assertions (the
           <span class="inlinecode">
            ASSERT_∗
           </span>
           macros), nonfatal assertions (the
           <span class="inlinecode">
            EXPECT_∗
           </span>
           macros), and so-called death tests (checks that a program terminates expectedly). Google Test also provides various options for running tests and offers textual and XML report generation. As I mentioned earlier, Google also provides a mock object testing framework, Google Mock, which integrates well with Google Test. The next code demonstrates the creation of a suite of unit tests using Google Test:
          </li>
         </ol>
        </div>
        <p class="textfl">
         <a id="p479">
         </a>
         <span aria-label="479" epub:type="pagebreak" id="pagebreak_479" role="doc-pagebreak">
         </span>
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0185">
           <img alt="image" height="1876" src="../../IMAGES/B9780443222191000210/main.assets/u12-33-9780443222191.jpg" width="1490"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <div>
         <ol id="ulist0060">
          <li class="bulllist" id="p0930">
           <a id="u0180">
           </a>
           ▪
           <b>
            Template Unit Test
           </b>
           (
           <a href="http://tut-framework.sourceforge.net/">
            http://tut-framework.sourceforge.net/
           </a>
           ): The Template Unit Test (TUT) Framework is a small, portable C++ unit test framework. It consists only of header files, so there's no library to link against or deploy. Tests are organized into named test groups and the framework supports automatic discovery of all tests that you define. Several test reporters are provided, including basic console output and a CppUnit-style reporter. It's also possible to write your own reporters using TUT's extensible reporter interface. Here is a simple canonical unit test written using the TUT framework:
          </li>
         </ol>
        </div>
        <p class="textfl">
         <a id="p480">
         </a>
         <span aria-label="480" epub:type="pagebreak" id="pagebreak_480" role="doc-pagebreak">
         </span>
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0190">
           <img alt="image" height="1586" src="../../IMAGES/B9780443222191000210/main.assets/u12-34-9780443222191.jpg" width="1318"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
       </section>
       <section>
        <a id="s0110">
        </a>
        <h3 class="h2hd" id="cesectitle0120">
         Code coverage
        </h3>
        <p class="textfl" id="p0935">
         Code coverage tools let you discover precisely which statements of your code are exercised by your tests (i.e., these tools can be used to focus your testing activities on the parts of your code base that are not already covered by tests). There are different degrees of code coverage that can be measured. I will define each of these with reference to this simple code example:
        </p>
        <div>
         <div class="pageavoid">
          <figure class="fig" id="f0195">
           <img alt="image" height="545" src="../../IMAGES/B9780443222191000210/main.assets/u12-35-9780443222191.jpg" width="1352"/>
           <figcaption class="figleg">
           </figcaption>
          </figure>
         </div>
        </div>
        <div>
         <ol id="ulist0065">
          <li class="bulllist" id="p0940">
           <a id="u0185">
           </a>
           ▪
           <b>
            Function coverage
           </b>
           . In this coarsest level of code coverage, only function calls are tracked. In our previous example, function coverage will record only whether
           <a id="p481">
           </a>
           <span aria-label="481" epub:type="pagebreak" id="pagebreak_481" role="doc-pagebreak">
           </span>
           <span class="inlinecode">
            TestFunction()
           </span>
           was called at least once. The flow of control within a function has no effect on function code coverage results.
          </li>
          <li class="bulllist" id="p0945">
           <a id="u0190">
           </a>
           ▪
           <b>
            Line coverage
           </b>
           . This form of code coverage tests whether each line of code that contains an executable statement was reached. One limitation of this metric can be seen on Line 1 of our previous code. Line coverage will consider Line 1 to be 100% exercised even if the
           <span class="inlinecode">
            a++
           </span>
           statement is not executed; it matters only if the flow of control hit this line. Obviously, you can get round this limitation by putting the
           <span class="inlinecode">
            if
           </span>
           condition and the
           <span class="inlinecode">
            a++
           </span>
           statement on separate lines.
          </li>
          <li class="bulllist" id="p0950">
           <a id="u0195">
           </a>
           ▪
           <b>
            Statement coverage
           </b>
           . This metric measures whether the flow of control reached every executable statement at least once. The primary limitation of this form of coverage is that it doesn't consider the different code paths that can result from the expressions in control structures such as
           <span class="inlinecode">
            if
           </span>
           ,
           <span class="inlinecode">
            for
           </span>
           ,
           <span class="inlinecode">
            while
           </span>
           , or
           <span class="inlinecode">
            switch
           </span>
           statements. For example, in our previous code, statement coverage will tell us if the condition on Line 3 evaluated to true, causing Line 4 to be executed. However, it will not tell us if that condition evaluated to false, because there is no executable code associated with that result.
          </li>
          <li class="bulllist" id="p0955">
           <a id="u0200">
           </a>
           ▪
           <b>
            Basic block coverage
           </b>
           . A basic block is a sequence of statements that cannot be branched into or out of. That is, if the first statement is executed, then all of the remaining statements in the block will also be executed. Essentially, a basic block ends on a branch, function call, throw, or return. This can be thought of as a special case of statement coverage, with the same benefits and limitations.
          </li>
          <li class="bulllist" id="p0960">
           <a id="u0205">
           </a>
           ▪
           <b>
            Decision coverage
           </b>
           . This code coverage metric measures whether the overall result of the expression in each control structure evaluated to both true and false. This addresses the major deficiency of statement coverage, because you will know if the condition in Line 3 evaluated to false. This is also called branch coverage.
          </li>
          <li class="bulllist" id="p0965">
           <a id="u0210">
           </a>
           ▪
           <b>
            Condition coverage
           </b>
           . Condition coverage determines whether each Boolean subexpression in a control structure has evaluated to both true and false. In our previous example, this means that Line 3 must be hit with
           <span class="inlinecode">
            a
           </span>
           &gt;10,
           <span class="inlinecode">
            a
           </span>
           ≤ 10,
           <span class="inlinecode">
            b
           </span>
           !
           <span title='hsp="0.25"'>
           </span>
           =
           <span title='hsp="0.25"'>
           </span>
           0, and
           <span class="inlinecode">
            b
           </span>
           <span title='hsp="0.25"'>
           </span>
           ==
           <span title='hsp="0.25"'>
           </span>
           0. This does not necessarily imply decision coverage, because each of these events could occur in such an order that the overall result of the if statement always evaluates to false.
          </li>
         </ol>
        </div>
        <p class="text" id="p0970">
         There are various programs that let you measure the code coverage of C++ code. Each of these supports different combinations of the previous metrics, normally by instrumenting the code that your compiler generates. Most of these tools are commercial offerings, although there are some free and open source options, too.
        </p>
        <p class="text" id="p0975">
         One feature in particular that can be useful is the ability to exclude certain lines of code from the analysis, which is often done by adding special comments to those lines of code. This can be used to turn off coverage for lines that legitimately can never be hit, such as methods in a base class that are always overridden, although in these cases it's important for the coverage tool to raise an error if that excluded code is ever exposed in the future.
         <a id="p482">
         </a>
        </p>
        <div>
         <span aria-label="482" epub:type="pagebreak" id="pagebreak_482" role="doc-pagebreak">
         </span>
        </div>
        <section>
         <a id="sf0020">
         </a>
         <div class="pageavoid">
          <figure class="fig" id="f0020">
           <img alt="image" height="786" src="../../IMAGES/B9780443222191000210/main.assets/f12-03-9780443222191.jpg" width="2734"/>
           <figcaption class="figleg">
            <a id="cap0020">
            </a>
            <a id="fspara0020">
            </a>
            <span class="fignum">
             <a href="#Bf0020">
              Figure 12.3
             </a>
            </span>
            Example HTML gcov code coverage report generated by lcov.
           </figcaption>
          </figure>
         </div>
        </section>
        <p class="text" id="p0980">
         Another issue to bear in mind is that you should normally perform code coverage analysis on a build that has been compiled without optimizations, because compilers can reorder or eliminate individual lines of code during optimization.
        </p>
        <p class="text" id="p0985">
         This list provides a brief survey of some of the more prominent code coverage analysis tools:
        </p>
        <div>
         <ol id="ulist0070">
          <li class="bulllist" id="p0990">
           <a id="u0215">
           </a>
           ▪
           <b>
            Gcov
           </b>
           (
           <a href="http://gcc.gnu.org/onlinedocs/gcc/Gcov.xhtml">
            http://gcc.gnu.org/onlinedocs/gcc/Gcov.xhtml
           </a>
           ). This test coverage program is part of the open-source GNU GCC compiler collection. It operates on code generated by
           <span class="inlinecode">
            g++
           </span>
           using the
           <span class="inlinecode">
            -fprofile-arcs
           </span>
           and
           <span class="inlinecode">
            -ftest-coverage
           </span>
           options. Gcov provides function, line, and branch code coverage. It outputs its report in a textual format; however, the accompanying lcov script can be used to output results as an HTML report (
           <a href="#f0020" id="Bf0020">
            Fig. 12.3
           </a>
           ).
          </li>
          <li class="bulllist" id="p0995">
           <a id="u0220">
           </a>
           ▪
           <b>
            Intel Code Coverage Tool
           </b>
           (
           <a href="http://www.intel.com/">
            http://www.intel.com/
           </a>
           ). This tool is included with Intel compilers and runs on instrumentation files produced by those compilers. It provides function and basic block coverage and can restrict analysis to only modules of interest. It also supports differential coverage (i.e., comparing the output of one run against another run). The Code-Coverage Tool runs on Intel processors under Windows or Linux.
          </li>
          <li class="bulllist" id="p1000">
           <a id="u0225">
           </a>
           ▪
           <b>
            Bullseye Coverage
           </b>
           (
           <a href="http://www.bullseye.com/">
            http://www.bullseye.com/
           </a>
           ). This coverage tool, from Bullseye Testing Technology, provides function as well as condition/decision coverage, to give you a range of coverage precision. It offers features such as covering system-level and kernel mode code, merging results from distributed testing, and integration with Microsoft Visual Studio. It also gives you the ability to exclude certain portions of your code from analysis. Bullseye is a mature product that has support for a wide range of platforms and compilers.
          </li>
          <li class="bulllist" id="p1005">
           <a id="u0230">
           </a>
           ▪
           <b>
            Rational PureCoverage
           </b>
           (
           <a href="http://www.rational.com/">
            http://www.rational.com/
           </a>
           ). This code coverage analysis tool is sold as part of the PurifyPlus package from UNICOM Systems. It can report coverage at the executable, library, file, function, block, and line levels. PureCoverage can accumulate coverage over multiple runs and merge data from different programs that share the same source code. It offers both graphical and textual output to let you explore its results.
          </li>
         </ol>
        </div>
        <p class="textfl">
         <a id="p483">
         </a>
        </p>
        <div>
         <span aria-label="483" epub:type="pagebreak" id="pagebreak_483" role="doc-pagebreak">
         </span>
        </div>
        <p class="text" id="p1010">
         Once you have a code coverage build in place and can refer to regular coverage reports for your API, you can start instituting code coverage targets for your code. For example, you might specify that all code must achieve a particular threshold, such as 75%, 90%, or 100% coverage. The target you select will depend greatly upon the coverage metric that you adopt: attaining 100% function coverage should be relatively easy, whereas 100% condition/decision coverage will be far more difficult. From experience, a high and respectable degree of code coverage would be 100% function, 90% line, or 75% condition coverage.
        </p>
        <p class="text" id="p1015">
         It's also worth specifically addressing the issue of code coverage and legacy code. Sometimes your API must depend upon a large amount of old code that has no test coverage at all. Michael Feathers defines legacy code as code without tests (
         <a href="../B9780443222191160015/References_583-587_B9780443222191160015.xhtml#bib29">
          Feathers, 2004
         </a>
         ). In these cases, it may be impractical to enforce the same code coverage targets for the legacy code that you impose for new code. However, you can at least put some basic tests in place and then enforce that no checkin should lower the current coverage level. This effectively means that any new changes to the legacy code should be accompanied with tests. Enforcing this requirement for each checkin can sometimes be difficult (because you have to build the software with the change and run all tests to know whether it should be accepted), so another reasonable way to make this work is to record the legacy code coverage for the previous version of the library and ensure that the coverage for the new version equals or exceeds this threshold at the time of release. This approach offers a pragmatic way to deal with legacy code and lets you gradually increase the code coverage to acceptable levels over time.
        </p>
        <p class="text" id="p1020">
         In essence, different modules or libraries in your API may have different code coverage targets. In the past, I've made this clear by updating the code coverage report to display the target for each module and use a color scheme to indicate whether the targets have been met in each case. You can then quickly glance down the report to know whether your testing levels are on target.
        </p>
       </section>
       <section>
        <a id="s0115">
        </a>
        <h3 class="h2hd" id="cesectitle0125">
         Bug tracking
        </h3>
        <p class="textfl" id="p1025">
         A bug tracking system is an application that lets you keep track of bugs (and often suggestions) for your software project in a database. An efficient bug tracking system that can be mapped well to your development and quality processes is an invaluable tool. Conversely, a poor bug tracking system that is difficult to use and does not fully reveal the state of your software can be an albatross around the neck of your project.
        </p>
        <p class="text" id="p1030">
         Most bug tracking systems support the triage of incoming bugs: that is, setting the priority (and perhaps severity) of a bug and assigning it to a particular developer. It's also standard practice to be able to define filters for the bugs in the system so that targeted bug lists can be created, such as a list of open crashing bugs or a list of bugs assigned to a particular developer. Related to this, some bug tracking systems will also provide report generation functions, often with the ability to display graphs and pie charts. This can be indispensable for generating quality metrics about your software, which together with
         <a id="p484">
         </a>
         <span aria-label="484" epub:type="pagebreak" id="pagebreak_484" role="doc-pagebreak">
         </span>
         code coverage results can be used to direct further testing efforts more efficiently. Another important feature is the ability to integrate your revision control system with your bug tracking system (e.g., so that bugs can be automatically marked as fixed or closed when you push a code change that fixes them).
        </p>
        <p class="text" id="p1035">
         It's also worth noting what a bug tracking system is not. For one, it is not a trouble ticket or issue tracking system. These are customer support systems that are used to receive feedback from users, many of which may not be related to software problems. Valid software issues that are discovered by customer support will then be entered into the bug tracking system and assigned to an engineer to work on. A bug tracking system is also generally not a task or project management tool: that is, a system that lets you track tasks and plan work. However, some bug tracking system vendors do provide complementary products that use the underlying infrastructure to provide a project management tool as well.
        </p>
        <p class="text" id="p1040">
         There are dozens of bug tracking systems on the market, and the best one for your project will depend upon several factors:
        </p>
        <div>
         <ol id="ulist0075">
          <li class="bulllist" id="p1045">
           <a id="u0235">
           </a>
           • You may prefer an open source solution so that you have the option to customize it if necessary. For example, many large open source projects use Bugzilla, including Mozilla, Gnome, Apache, and the Linux Kernel (see
           <a href="https://www.bugzilla.org/">
            https://www.bugzilla.org/
           </a>
           ).
          </li>
          <li class="bulllist" id="p1050">
           <a id="u0240">
           </a>
           • There are also many commercial packages that provide excellent and flexible bug tracking capabilities that come with support and optionally secure hosting. Atlassian's JIRA is one such popular solution that provides an extremely customizable and robust bug tracking system. Atlassian also provides the related GreenHopper project management system for agile development projects, which lets you manage your user story backlog, task breakdowns, and sprint/iteration planning (see
           <a href="https://www.atlassian.com/">
            https://www.atlassian.com/
           </a>
           ).
          </li>
          <li class="bulllist" id="p1055">
           <a id="u0245">
           </a>
           • Alternatively, you may decide to go with a general project hosting solution that provides revision control features, disk storage quotas, discussion forums, and an integrated bug tracking system. GitHub is one popular option in this category (see
           <a href="https://github.com/">
            https://github.com/
           </a>
           ).
          </li>
         </ol>
        </div>
       </section>
       <section>
        <a id="s0120">
        </a>
        <h3 class="h2hd" id="cesectitle0130">
         Continuous build system
        </h3>
        <p class="textfl" id="p1060">
         A continuous build system is an automated process that rebuilds your software as soon as a new change is checked into your revision control system. This is also commonly known as continuous integration (CI) and continuous delivery (CD). A continuous build system should be one of the first pieces of technology you put in place for a large project with multiple engineers, independent of any testing needs. It lets you know the current state of the build and identifies build failures as soon as they happen. It is also invaluable for cross-platform development, because even the most well-tested change for one platform can break the build for a different platform. There are several continuous build options on the market, including:
         <a id="p485">
         </a>
         <span aria-label="485" epub:type="pagebreak" id="pagebreak_485" role="doc-pagebreak">
         </span>
        </p>
        <div>
         <ol id="ulist0080">
          <li class="bulllist" id="p1065">
           <a id="u0250">
           </a>
           •
           <b>
            Jenkins
           </b>
           (
           <a href="https://www.jenkins.io/">
            https://www.jenkins.io/
           </a>
           ): An open source automation server for building, testing, and deploying software projects.
          </li>
          <li class="bulllist" id="p1070">
           <a id="u0255">
           </a>
           •
           <b>
            TeamCity
           </b>
           (
           <a href="https://www.jetbrains.com/teamcity/">
            https://www.jetbrains.com/teamcity/
           </a>
           ): A distributed build management and CI server from JetBrains.
          </li>
          <li class="bulllist" id="p1075">
           <a id="u0260">
           </a>
           •
           <b>
            CircleCI
           </b>
           (
           <a href="https://circleci.com/">
            https://circleci.com/
           </a>
           ): A CI/CD platform that can be used to implement DevOps practices.
          </li>
          <li class="bulllist" id="p1080">
           <a id="u0265">
           </a>
           •
           <b>
            Travis CI
           </b>
           (
           <a href="https://travis-ci.com/">
            https://travis-ci.com/
           </a>
           ): A hosted CI service that can build and test software on popular source code hosting services such as GitHub.
          </li>
         </ol>
        </div>
        <p class="text" id="p1085">
         However, our focus here is on automated testing. The benefit of a continuous build system for testing is that it also provides a mechanism for running your automated tests regularly and hence discovering test breakages quickly. That is, the result of the automated build can be in one of four states: in progress, pass, build failure, or test failure.
        </p>
        <p class="text" id="p1090">
         As your test suite grows, so will the time it takes to run all of your tests. It's important that you receive fast turnaround on builds, so if your test run starts taking several hours to complete, then you should investigate some test optimization efforts. One way to do this is to segment your tests into different categories and run only the fast category of tests as part of the continuous build. Another solution is to have multiple automated builds: one that only builds the software and another that builds the software and then runs all tests. This gives engineers the ability to receive feedback quickly about build breakages while ensuring that a full test suite is run as often as possible.
        </p>
       </section>
      </section>
     </section>
    </section>
   </div>
  </div>
 </body>
</html>
